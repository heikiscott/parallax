# Parallax Evaluation Framework

A unified, modular evaluation framework for benchmarking memory systems on standard datasets.

## üåü Key Features

### Unified & Modular Framework
- **One codebase for all**: No need to write separate code for each dataset or system
- **Plug-and-play systems**: Support multiple memory systems (Parallax, mem0, memOS, memU, etc.)
- **Multiple benchmarks**: LoCoMo, LongMemEval, PersonaMem out of the box
- **Consistent evaluation**: All systems evaluated with the same pipeline and metrics

### Automatic Compatibility Detection
The framework automatically detects and adapts to:
- **Multi-user vs Single-user conversations**: Handles both conversation types seamlessly
- **Q&A vs Multiple-choice questions**: Adapts evaluation approach based on question format  
- **With/without timestamps**: Works with or without temporal information

### Robust Checkpoint System
- **Cross-stage checkpoints**: Resume from any pipeline stage (add ‚Üí search ‚Üí answer ‚Üí evaluate)
- **Fine-grained resume**: Saves progress every conversation (search) and every 400 questions (answer)


## üèóÔ∏è Architecture Overview

### Code Structure

```
eval/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/           # Pipeline orchestration and data models
‚îÇ   ‚îú‚îÄ‚îÄ adapters/       # System-specific implementations
‚îÇ   ‚îú‚îÄ‚îÄ evaluators/     # Answer evaluation (LLM Judge, Exact Match)
‚îÇ   ‚îú‚îÄ‚îÄ converters/     # Dataset format converters
‚îÇ   ‚îî‚îÄ‚îÄ utils/          # Configuration, logging, I/O
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ datasets/       # Dataset configurations (locomo.yaml, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ systems/        # System configurations (parallax.yaml, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ prompts.yaml    # Prompt templates
‚îú‚îÄ‚îÄ data/               # Benchmark datasets
‚îî‚îÄ‚îÄ results/            # Evaluation results and logs
```

### Pipeline Flow

The evaluation consists of 4 sequential stages:

1. **Add**: Ingest conversations and build indexes
2. **Search**: Retrieve relevant memories for each question
3. **Answer**: Generate answers using retrieved context
4. **Evaluate**: Assess answer quality with LLM Judge or Exact Match

Each stage saves its output and can be resumed independently.

## üöÄ Getting Started

### Prerequisites

- Python 3.10+
- Parallax environment configured (see main project's `env.template`)

### Data Preparation

Place your dataset files in the `evaluation/data/` directory:

**LoCoMo** (native format, no conversion needed):

```
evaluation/data/locomo/
‚îî‚îÄ‚îÄ locomo10.json
```

**LongMemEval** (auto-converts to LoCoMo format):

```
evaluation/data/longmemeval/
‚îî‚îÄ‚îÄ longmemeval_s_cleaned.json  # Original file
# ‚Üí Will auto-generate: longmemeval_s_locomo_style.json
```

**PersonaMem** (auto-converts to LoCoMo format):

```
evaluation/data/personamem/
‚îú‚îÄ‚îÄ questions_32k.csv           # Original file
‚îî‚îÄ‚îÄ shared_contexts_32k.jsonl   # Original file
# ‚Üí Will auto-generate: personamem_32k_locomo_style.json
```

The framework will automatically detect and convert non-LoCoMo formats on first run. You don't need to manually run any conversion scripts.

### Installation

Install evaluation-specific dependencies:

```bash
# For evaluating local systems (Parallax)
uv sync --group evaluation

# For evaluating online API systems (mem0, memOS, memU, etc.)
uv sync --group evaluation-full
```

### Environment Configuration

The evaluation framework reuses most environment variables from the main Parallax `.env` file:
- `LLM_API_KEY`, `LLM_BASE_URL` (for answer generation)
- `DEEPINFRA_API_KEY` (for embeddings/reranker)

For testing Parallax, please first configure the whole .env file.

**Additional variables for online API systems** (add to `.env` if testing these systems):

```bash
# Mem0
MEM0_API_KEY=your_mem0_api_key

# memOS
MEMOS_KEY=your_memos_api_key

# memU
MEMU_API_KEY=your_memu_api_key
```

### Quick Test (Mini Dataset)

Run a quick test with the mini dataset to verify everything works:

```bash
# Navigate to project root
cd /path/to/memsys-opensource

# Run mini dataset (single conversation with limited questions)
uv run python -m evaluation.cli --dataset locomo-mini --system parallax

# Or use the shorthand script
python evaluation/run_locomo.py --mini
```

### Test Single Conversation

Test a specific conversation by index:

```bash
# Test conversation at index 3
uv run python -m evaluation.cli --dataset locomo --system parallax --conv 3

# Or use the shorthand script
python evaluation/run_locomo.py --conv 3
```

### Full Evaluation

Run the complete benchmark on all conversations:

```bash
# Evaluate Parallax on LoCoMo (all 10 conversations)
uv run python -m evaluation.cli --dataset locomo --system parallax

# Or use the shorthand script
python evaluation/run_locomo.py --all

# Evaluate other systems
uv run python -m evaluation.cli --dataset locomo --system memos
uv run python -m evaluation.cli --dataset locomo --system memu
# For mem0, it's recommended to run add first, check the memory status on the web console to make sure it's finished and then following stages.
uv run python -m evaluation.cli --dataset locomo --system mem0 --stages add
uv run python -m evaluation.cli --dataset locomo --system mem0 --stages search answer evaluate

# Evaluate on other datasets
uv run python -m evaluation.cli --dataset longmemeval --system parallax
uv run python -m evaluation.cli --dataset personamem --system parallax

# Use --run-name to distinguish multiple runs (useful for A/B testing)
# Results will be saved to: results/{dataset}-{system}-{run-name}/
uv run python -m evaluation.cli --dataset locomo --system parallax --run-name baseline
uv run python -m evaluation.cli --dataset locomo --system parallax --run-name experiment1
uv run python -m evaluation.cli --dataset locomo --system parallax --run-name 20241107

# Resume from checkpoint if interrupted (automatic)
# Just re-run the same command - it will detect and resume from checkpoint
uv run python -m evaluation.cli --dataset locomo --system parallax

```

### View Results

Results are saved to `evaluation/results/{dataset}-{system}[-{run-name}]/`:

```bash
# View summary report
cat evaluation/results/locomo-parallax/report.txt

# View detailed evaluation results
cat evaluation/results/locomo-parallax/eval_results.json

# View pipeline execution log
cat evaluation/results/locomo-parallax/pipeline.log
```

**Result files:**
- `report.txt` - Summary metrics (accuracy, total questions)
- `eval_results.json` - Per-question evaluation details
- `answer_results.json` - Generated answers and retrieved context
- `search_results.json` - Retrieved memories for each question
- `pipeline.log` - Detailed execution logs

## üìä Understanding Results

### Metrics

- **Accuracy**: Percentage of correct answers (as judged by LLM)
- **Total Questions**: Number of questions evaluated
- **Correct**: Number of questions answered correctly

### Detailed Results

Check `eval_results.json` for per-question breakdown:

**LoCoMo example (Q&A format, evaluated by LLM Judge):**

```json
{
  "total_questions": ...,
  "correct": ...,
  "accuracy": ...,
  "detailed_results": {
      "locomo_exp_user_0": [
         {
            "question_id": "locomo_0_qa0",
            "question": "What is my favorite food?",
            "golden_answer": "Pizza",
            "generated_answer": "Your favorite food is pizza.",
            "judgments": [
               true,
               true,
               true
            ],
            "category": "1"
         }
         ...
      ]
  }
}
```

**PersonaMem example (Multiple-choice format, evaluated by Exact Match):**

```json
{
  "overall_accuracy": ...,
  "total_questions": ...,
  "correct_count": ...,
  "detailed_results": [
    {
      "question_id": "acd74206-37dc-4756-94a8-b99a395d9a21",
      "question": "I recently attended an event where there was a unique blend of modern beats with Pacific sounds.",
      "golden_answer": "(c)",
      "generated_answer": "(c)",
      "is_correct": true,
      "category": "recall_user_shared_facts"
    }
    ...
  ]
}
```

## üîß Advanced Usage

### Run Specific Stages

Skip completed stages to iterate faster:

```bash
# Only run search stage (if add is already done)
uv run python -m evaluation.cli --dataset locomo --system parallax --stages search

# Run search, answer, and evaluate (skip add)
uv run python -m evaluation.cli --dataset locomo --system parallax \
    --stages search answer evaluate
```
If you have already done search, and you want to do it again, please remove the "search" (and following stages from the completed_stages in the checkpoint_default.json file):
```
  "completed_stages": [
    "answer",
    "search",
    "evaluate",
    "add"
  ]
```


### Custom Configuration

Modify system or dataset configurations:

```bash
# Copy and edit configuration
cp evaluation/config/systems/parallax.yaml evaluation/config/systems/parallax_custom.yaml
# Edit parallax_custom.yaml with your changes

# Run with custom config
uv run python -m evaluation.cli --dataset locomo --system parallax_custom
```


## üîå Supported Systems

### Local Systems
- **parallax** - Parallax with MemCell extraction and dual-mode retrieval

### Online API Systems
- **mem0** - Mem0 API
- **memos** - memOS API  
- **memu** - memU HTTP API

## üìö Supported Datasets

- **locomo** - LoCoMo: Long-term Conversation Memory benchmark
- **longmemeval** - LongMemEval: Extended conversation evaluation
- **personamem** - PersonaMem: Persona consistency evaluation

## üìÑ License

Same as the parent project.

