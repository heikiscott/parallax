"""Stage 1.5: Group Event Clustering

å¯¹ MemUnits è¿›è¡Œ LLM é©±åŠ¨çš„äº‹ä»¶èšç±»ã€‚
è¿è¡Œåœ¨ Add é˜¶æ®µä¹‹åï¼ŒSearch é˜¶æ®µä¹‹å‰ã€‚
"""
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional

from rich.console import Console
from rich.progress import (
    Progress,
    SpinnerColumn,
    TextColumn,
    BarColumn,
    MofNCompleteColumn,
    TaskProgressColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)

from memory.group_event_cluster import (
    GroupEventClusterer,
    GroupEventClusterConfig,
    JsonClusterStorage,
)
from providers.llm.llm_provider import LLMProvider


async def run_group_event_clustering(
    conversations: List[Any],
    memunits_dir: Path,
    clusters_dir: Path,
    config: dict,
    checkpoint_manager: Optional[Any] = None,
) -> Dict[str, Any]:
    """
    æ‰§è¡Œç¾¤ä½“äº‹ä»¶èšç±»

    Args:
        conversations: å¯¹è¯åˆ—è¡¨
        memunits_dir: MemUnits ç›®å½•
        clusters_dir: èšç±»ç»“æœè¾“å‡ºç›®å½•
        config: é…ç½®å­—å…¸ï¼ˆåŒ…å« llm, group_event_cluster_config ç­‰ï¼‰
        checkpoint_manager: æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨

    Returns:
        èšç±»ç»“æœå­—å…¸ï¼ŒåŒ…å« cluster_indices
    """
    console = Console()
    clusters_dir.mkdir(parents=True, exist_ok=True)

    # ========== Stage 1.5: Group Event Clustering ==========
    console.print(f"\n{'='*60}", style="bold cyan")
    console.print(f"Stage 1.5: Group Event Clustering", style="bold cyan")
    console.print(f"{'='*60}", style="bold cyan")

    # è·å–èšç±»é…ç½®
    cluster_config_dict = config.get('group_event_cluster_config', {})
    llm_cfg = config.get('llm', {})

    # åˆ›å»º GroupEventClusterConfig
    cluster_config = GroupEventClusterConfig(
        llm_provider=cluster_config_dict.get("llm_provider", "openai"),
        llm_model=cluster_config_dict.get("llm_model") or llm_cfg.get("model", "gpt-4o-mini"),
        llm_api_key=cluster_config_dict.get("llm_api_key") or llm_cfg.get("api_key"),
        llm_base_url=cluster_config_dict.get("llm_base_url") or llm_cfg.get("base_url"),
        llm_temperature=cluster_config_dict.get("llm_temperature", 0.0),
        summary_update_threshold=cluster_config_dict.get("summary_update_threshold", 5),
        max_clusters_in_prompt=cluster_config_dict.get("max_clusters_in_prompt", 20),
        max_members_per_cluster_in_prompt=cluster_config_dict.get("max_members_per_cluster_in_prompt", 3),
        output_dir=clusters_dir,
    )

    console.print(f"ğŸ¤– Cluster LLM Model: {cluster_config.llm_model}", style="cyan")

    # åˆ›å»º LLM provider
    llm_provider = LLMProvider(
        provider_type=cluster_config.llm_provider,
        model=cluster_config.llm_model,
        api_key=cluster_config.llm_api_key,
        base_url=cluster_config.llm_base_url,
        temperature=cluster_config.llm_temperature,
        max_tokens=cluster_config.llm_max_tokens,
    )

    # åˆ›å»º clusterer å’Œ storage
    clusterer = GroupEventClusterer(
        config=cluster_config,
        llm_provider=llm_provider,
    )
    storage = JsonClusterStorage(output_dir=clusters_dir)

    # æ£€æŸ¥å·²å®Œæˆçš„ä¼šè¯ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    # æ³¨ï¼šæ–‡ä»¶æœ¬èº«å°±æ˜¯ checkpointï¼Œæ— è®º checkpoint_manager æ˜¯å¦å­˜åœ¨éƒ½è¦æ£€æŸ¥
    all_conv_ids = [i for i in range(len(conversations))]
    completed_convs = load_cluster_progress(clusters_dir, all_conv_ids)

    # è¿‡æ»¤å‡ºå¾…å¤„ç†çš„ä¼šè¯
    pending_conv_ids = [i for i in all_conv_ids if i not in completed_convs]

    console.print(f"\nğŸ“Š æ€»ä¼šè¯æ•°: {len(conversations)}", style="bold cyan")
    console.print(f"âœ… å·²å®Œæˆ: {len(completed_convs)}", style="bold green")
    console.print(f"â³ å¾…å¤„ç†: {len(pending_conv_ids)}", style="bold yellow")

    # å¤„ç†ç»“æœ
    cluster_indices = {}
    total_clusters = 0
    total_units = 0

    # åŠ è½½å·²å®Œæˆçš„èšç±»ç´¢å¼•
    for conv_idx in completed_convs:
        conv_id = f"conv_{conv_idx}"
        existing_index = await storage.load_index(conv_id)
        if existing_index:
            cluster_indices[conv_id] = existing_index
            total_clusters += len(existing_index.clusters)
            total_units += existing_index.total_units

    if len(pending_conv_ids) == 0:
        console.print(f"\nğŸ‰ æ‰€æœ‰ä¼šè¯å·²å®Œæˆèšç±»ï¼Œè·³è¿‡ï¼", style="bold green")
    else:
        console.print(f"ğŸš€ å¼€å§‹å¤„ç†...\n", style="bold green")

        start_time = time.time()

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            MofNCompleteColumn(),
            TextColumn("â€¢"),
            TaskProgressColumn(),
            TextColumn("â€¢"),
            TimeElapsedColumn(),
            TextColumn("â€¢"),
            TimeRemainingColumn(),
            console=console,
            transient=False,
        ) as progress:
            task = progress.add_task(
                "[cyan]Clustering conversations...",
                total=len(pending_conv_ids),
            )

            for conv_idx in pending_conv_ids:
                conv_id = f"conv_{conv_idx}"
                # å…¼å®¹å†å²ä¸å½“å‰æ–‡ä»¶å‘½å
                memunit_candidates = [
                    memunits_dir / f"memunit_list_conv_{conv_idx}.json",
                    memunits_dir / f"memunits_conv_{conv_idx}.json",
                ]
                memunits_path = next((p for p in memunit_candidates if p.exists()), None)

                progress.update(task, description=f"[cyan]Processing {conv_id}...")

                # åŠ è½½ MemUnits
                if memunits_path is None:
                    console.print(f"  âš ï¸  {conv_id}: memunits file not found, skipping", style="yellow")
                    progress.advance(task)
                    continue

                try:
                    with open(memunits_path, "r", encoding="utf-8") as f:
                        memunits_data = json.load(f)

                    # å¤„ç†ä¸åŒæ ¼å¼
                    if isinstance(memunits_data, list):
                        memunits = memunits_data
                    elif isinstance(memunits_data, dict):
                        memunits = memunits_data.get("memunits", list(memunits_data.values()))
                    else:
                        memunits = []

                    if not memunits:
                        console.print(f"  âš ï¸  {conv_id}: no memunits found, skipping", style="yellow")
                        progress.advance(task)
                        continue

                    # æ‰§è¡Œèšç±»
                    index = await clusterer.cluster_memunits(
                        memunit_list=memunits,
                        conversation_id=conv_id,
                    )

                    # ä¿å­˜ç»“æœï¼ˆcheckpoint: æ¯ä¸ªä¼šè¯å®Œæˆåç«‹å³ä¿å­˜ï¼‰
                    await storage.save_index(conv_id, index)

                    cluster_indices[conv_id] = index
                    total_clusters += len(index.clusters)
                    total_units += index.total_units

                    stats = index.get_stats()
                    console.print(
                        f"  âœ“ {conv_id}: {stats['total_clusters']} clusters, "
                        f"{stats['indexed_units']} units, avg size {stats['avg_cluster_size']:.1f}",
                        style="green"
                    )

                except Exception as e:
                    console.print(f"  âŒ {conv_id}: clustering failed - {e}", style="red")

                progress.advance(task)

        elapsed_time = time.time() - start_time
        console.print(f"\nâ±ï¸  è€—æ—¶: {elapsed_time:.2f}s", style="dim")

    # ä¿å­˜æ±‡æ€»ä¿¡æ¯
    config_dict = cluster_config.to_dict()
    # è¿‡æ»¤æ•æ„Ÿå­—æ®µï¼Œé¿å…åœ¨ç»“æœæ–‡ä»¶ä¸­æ³„æ¼å‡­æ®
    if "llm_api_key" in config_dict:
        config_dict["llm_api_key"] = "[redacted]"

    summary = {
        "total_conversations": len(conversations),
        "clustered_conversations": len(cluster_indices),
        "total_clusters": total_clusters,
        "total_units": total_units,
        "config": config_dict,
    }
    summary_path = clusters_dir / "clustering_summary.json"
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2, default=str)

    console.print(f"\nğŸ“Š Clustering complete: {total_clusters} clusters, {total_units} units", style="cyan")
    console.print(f"{'='*60}\n", style="dim")

    return {"cluster_indices": cluster_indices}


def load_cluster_progress(clusters_dir: Path, all_conv_ids: list) -> set:
    """
    åŠ è½½ Cluster é˜¶æ®µçš„ç»†ç²’åº¦è¿›åº¦ï¼ˆæ£€æŸ¥å“ªäº›ä¼šè¯å·²å®Œæˆï¼‰

    Args:
        clusters_dir: èšç±»ç»“æœç›®å½•
        all_conv_ids: æ‰€æœ‰ä¼šè¯ ID åˆ—è¡¨

    Returns:
        å·²å®Œæˆçš„ä¼šè¯ ID é›†åˆï¼ˆæ•°å­—ç´¢å¼•ï¼‰
    """
    completed_convs = set()

    if not clusters_dir.exists():
        return completed_convs

    for conv_idx in all_conv_ids:
        conv_id = f"conv_{conv_idx}"
        # JsonClusterStorage çš„æ–‡ä»¶å‘½åæ ¼å¼
        cluster_file = clusters_dir / f"{conv_id}.json"

        if cluster_file.exists():
            try:
                with open(cluster_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    # éªŒè¯æ–‡ä»¶æœ‰æ•ˆæ€§
                    if data and "clusters" in data:
                        completed_convs.add(conv_idx)
            except Exception:
                pass  # æ–‡ä»¶æŸåï¼Œå°†é‡æ–°å¤„ç†

    return completed_convs
