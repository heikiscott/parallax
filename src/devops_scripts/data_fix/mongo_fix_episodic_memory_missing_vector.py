#!/usr/bin/env python3
"""
ä¿®å¤å†å² EpisodicMemory æ–‡æ¡£ä¸­ç¼ºå¤±çš„å‘é‡å­—æ®µã€‚

è¿è¡Œæ–¹å¼ï¼ˆæ¨èé€šè¿‡ bootstrap è¿è¡Œï¼Œè‡ªåŠ¨åŠ è½½åº”ç”¨ä¸Šä¸‹æ–‡ä¸ä¾èµ–æ³¨å…¥ï¼‰ï¼š
  python src/bootstrap.py src/scripts/data_fix/fix_episodic_memory_missing_vector.py --limit 1000 --batch 200 --concurrency 8

å‚æ•°ï¼š
  --limit         æœ€å¤šå¤„ç†çš„æ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤ 1000ï¼‰
  --batch         æ¯æ¬¡ä»æ•°æ®åº“æ‹‰å–çš„æ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤ 200ï¼Œè¶Šå¤§è¶Šå¿«ä½†æ›´å å†…å­˜ï¼‰
  --concurrency   å¹¶å‘åº¦ï¼ˆé»˜è®¤ 8ï¼‰
"""

import argparse
import asyncio
from typing import Any, Dict, List, Optional, Tuple

from core.observation.logger import get_logger
from infra_layer.adapters.out.persistence.document.memory.episodic_memory import (
    EpisodicMemory,
)
from agentic_layer.vectorize_service import get_vectorize_service
from common_utils.datetime_utils import from_iso_format, to_iso_format


logger = get_logger(__name__)

# ç›®æ ‡å‘é‡æ¨¡å‹ï¼šä¸ç­‰äºè¯¥æ¨¡å‹çš„è®°å½•ä¹Ÿéœ€è¦é‡åˆ·
TARGET_VECTOR_MODEL = "Qwen/Qwen3-Embedding-4B"


async def _fetch_candidates(
    size: int,
    created_before: Optional[Any],
    created_gte: Optional[Any],
    created_lte: Optional[Any],
) -> List[EpisodicMemory]:
    """
    æŸ¥è¯¢ç¼ºå¤±å‘é‡çš„æƒ…æ™¯è®°å¿†å€™é€‰æ–‡æ¡£ã€‚

    è¿”å›ä»¥ä¸‹ä¸¤ç±»æ–‡æ¡£ï¼š
    1) episode ä¸ä¸ºç©ºä¸” vector ä¸å­˜åœ¨/ä¸º None/ä¸ºç©ºæ•°ç»„ çš„æ–‡æ¡£
    2) vector_model ä¸ç­‰äºç›®æ ‡æ¨¡å‹ï¼ˆTARGET_VECTOR_MODELï¼‰ çš„æ–‡æ¡£ï¼ˆå³éœ€è¦é‡åˆ·ï¼‰
    """
    and_filters: List[Dict[str, Any]] = [
        {"episode": {"$exists": True, "$ne": ""}},
        {
            "$or": [
                {"vector": {"$exists": False}},
                {"vector": None},
                {"vector": []},
                {"vector_model": {"$ne": TARGET_VECTOR_MODEL}},
                {"vector_model": {"$exists": False}},
                {"vector_model": None},
                {"vector_model": ""},
            ]
        },
    ]

    # created_at è¿‡æ»¤æ¡ä»¶ï¼ˆèŒƒå›´ + ç¿»é¡µé”šç‚¹ï¼‰
    created_at_filter: Dict[str, Any] = {}
    if created_gte is not None:
        created_at_filter["$gte"] = created_gte
    if created_lte is not None:
        created_at_filter["$lte"] = created_lte
    # ç¿»é¡µé”šç‚¹ï¼šä¼˜å…ˆå¤„ç†æœ€è¿‘åˆ›å»ºçš„æ•°æ®ï¼Œå…¶æ¬¡æŒ‰æ›´æ—©çš„æ•°æ®ç»§ç»­ç¿»é¡µ
    if created_before is not None:
        created_at_filter["$lt"] = created_before
    if created_at_filter:
        and_filters.append({"created_at": created_at_filter})

    query: Dict[str, Any] = {"$and": and_filters}

    cursor = EpisodicMemory.find(query).sort("-created_at").limit(size)  # æœ€è¿‘ä¼˜å…ˆ

    results = await cursor.to_list()
    return results


async def _process_one(
    document: EpisodicMemory, semaphore: asyncio.Semaphore
) -> Tuple[Optional[str], Optional[str]]:
    """
    å¤„ç†å•ä¸ªæ–‡æ¡£ï¼šå‘é‡åŒ– episode å¹¶å›å†™ vector ä¸ vector_modelã€‚

    è¿”å› (doc_id, error)ï¼›æˆåŠŸæ—¶ error ä¸º Noneã€‚
    """
    async with semaphore:
        try:
            if not document.episode:
                return str(document.id), "episode ä¸ºç©ºï¼Œè·³è¿‡"

            vectorize_service = get_vectorize_service()
            embedding = await vectorize_service.get_embedding(document.episode)
            vector_list = embedding.tolist()  # ä¸ä»“åº“é€»è¾‘ä¿æŒä¸€è‡´
            model_name = vectorize_service.get_model_name()

            # ç²¾ç¡®æŒ‰ _id æ›´æ–°ï¼Œé¿å…è¦†ç›–å…¶ä»–å­—æ®µ
            await EpisodicMemory.find({"_id": document.id}).update(
                {"$set": {"vector": vector_list, "vector_model": model_name}}
            )

            return str(document.id), None
        except Exception as exc:  # noqa: BLE001 éå…³é”®é”™è¯¯ï¼Œè®°å½•åç»§ç»­
            return str(document.id), str(exc)


async def run_fix(
    limit: int = 1000,
    batch: int = 200,
    concurrency: int = 10,
    start_created_at: Optional[Any] = None,
    end_created_at: Optional[Any] = None,
) -> Dict[str, Any]:
    """
    æ‰§è¡Œä¿®å¤ä»»åŠ¡ã€‚

    Args:
        limit:    æœ€å¤šå¤„ç†çš„æ–‡æ¡£æ•°é‡
        batch:    æ¯æ¬¡æ‰¹é‡ä»æ•°æ®åº“æ‹‰å–çš„æ–‡æ¡£æ•°é‡
        concurrency: å¹¶å‘åº¦ï¼ˆåç¨‹å¹¶å‘ï¼‰

    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    if limit <= 0:
        limit = 1
    if batch <= 0:
        batch = 1
    if concurrency <= 0:
        concurrency = 1

    semaphore = asyncio.Semaphore(concurrency)

    processed_total = 0
    succeeded = 0
    errors: List[Tuple[str, str]] = []
    created_before: Optional[Any] = None
    # é€šè¿‡å‡½æ•°å‚æ•°ä¼ å…¥çš„èŒƒå›´è¿‡æ»¤
    created_gte: Optional[Any] = start_created_at
    created_lte: Optional[Any] = end_created_at

    logger.info(
        "ğŸ” å¼€å§‹æ‰«æéœ€ä¿®å¤æ–‡æ¡£ï¼ˆlimit=%d, batch=%d, concurrency=%dï¼‰",
        limit,
        batch,
        concurrency,
    )

    while processed_total < limit:
        fetch_size = min(batch, limit - processed_total)
        candidates = await _fetch_candidates(
            size=fetch_size,
            created_before=created_before,
            created_gte=created_gte,
            created_lte=created_lte,
        )

        if not candidates:
            break

        # ä¸‹ä¸€é¡µé”šç‚¹ï¼šæœ¬æ‰¹æ¬¡ä¸­æœ€æ—©çš„ created_at
        try:
            created_before = candidates[-1].created_at
            try:
                logger.info("â±ï¸ å½“å‰å¤„ç†åˆ° created_at=%s", to_iso_format(created_before))
            except Exception:  # noqa: BLE001
                logger.info("â±ï¸ å½“å‰å¤„ç†åˆ° created_at=%s", str(created_before))
        except AttributeError:
            # å¦‚æœæ¨¡å‹æ— è¯¥å­—æ®µæˆ–å¼‚å¸¸ï¼Œé€€åŒ–ä¸ºæŒ‰ skip é€»è¾‘ï¼ˆä¸æ›´æ–°é”šç‚¹ï¼‰
            pass

        logger.info(
            "ğŸ“¦ æ‹‰å–åˆ°å€™é€‰ %d æ¡ï¼ˆå·²ç´¯è®¡å¤„ç†=%d/%dï¼‰",
            len(candidates),
            processed_total,
            limit,
        )

        tasks: List[asyncio.Task] = []
        for doc in candidates:
            task = asyncio.create_task(_process_one(doc, semaphore))
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=False)

        for doc_id, err in results:
            if err is None:
                succeeded += 1
            else:
                errors.append((doc_id or "unknown", err))

        processed_total += len(candidates)

    failed = len(errors)
    if failed:
        for doc_id, err_msg in errors[:20]:  # é¿å…æ—¥å¿—è¿‡å¤š
            logger.error("âŒ ä¿®å¤å¤±è´¥ doc=%s, error=%s", doc_id, err_msg)
        if failed > 20:
            logger.error("â€¦ è¿˜æœ‰ %d æ¡é”™è¯¯æœªé€æ¡æ‰“å°", failed - 20)

    logger.info(
        "âœ… ä¿®å¤å®Œæˆ | total=%d, succeeded=%d, failed=%d",
        processed_total,
        succeeded,
        failed,
    )
    return {
        "total": processed_total,
        "succeeded": succeeded,
        "failed": failed,
        "errors": errors,
    }


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="ä¿®å¤å†å² EpisodicMemory ç¼ºå¤±å‘é‡æ•°æ®")
    parser.add_argument(
        "--limit", type=int, default=1000, help="æœ€å¤šå¤„ç†çš„æ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤ 1000ï¼‰"
    )
    parser.add_argument(
        "--batch", type=int, default=200, help="æ¯æ¬¡ä»æ•°æ®åº“æ‹‰å–çš„æ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤ 200ï¼‰"
    )
    parser.add_argument("--concurrency", type=int, default=8, help="å¹¶å‘åº¦ï¼ˆé»˜è®¤ 8ï¼‰")
    parser.add_argument(
        "--start-created-at",
        dest="start_created_at",
        type=str,
        default=None,
        help="ä»…å¤„ç† created_at å¤§äºç­‰äºè¯¥æ—¶é—´çš„æ–‡æ¡£ï¼ˆISOæ ¼å¼ï¼Œä¾‹å¦‚ 2025-09-16T20:20:06+08:00ï¼‰",
    )
    parser.add_argument(
        "--end-created-at",
        dest="end_created_at",
        type=str,
        default=None,
        help="ä»…å¤„ç† created_at å°äºç­‰äºè¯¥æ—¶é—´çš„æ–‡æ¡£ï¼ˆISOæ ¼å¼ï¼Œä¾‹å¦‚ 2025-09-30T23:59:59+08:00ï¼‰",
    )
    return parser.parse_args()


def main():
    args = _parse_args()
    # é€šè¿‡ bootstrap è¿è¡Œæ—¶ï¼Œåº”ç”¨ä¸Šä¸‹æ–‡å·²åŠ è½½ï¼›æ­¤å¤„ç›´æ¥æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
    # è§£ææ—¶é—´èŒƒå›´å‚æ•°ï¼ˆISO -> å¸¦æ—¶åŒº datetimeï¼‰
    start_dt = from_iso_format(args.start_created_at) if args.start_created_at else None
    end_dt = from_iso_format(args.end_created_at) if args.end_created_at else None

    if start_dt or end_dt:
        try:
            start_str = to_iso_format(start_dt) if start_dt else "(æœªæŒ‡å®š)"
            end_str = to_iso_format(end_dt) if end_dt else "(æœªæŒ‡å®š)"
        except Exception:  # noqa: BLE001
            start_str = str(start_dt) if start_dt else "(æœªæŒ‡å®š)"
            end_str = str(end_dt) if end_dt else "(æœªæŒ‡å®š)"
        logger.info("â›³ ä½¿ç”¨ created_at è¿‡æ»¤èŒƒå›´: [%s, %s]", start_str, end_str)

    asyncio.run(
        run_fix(
            limit=args.limit,
            batch=args.batch,
            concurrency=args.concurrency,
            start_created_at=start_dt,
            end_created_at=end_dt,
        )
    )


if __name__ == "__main__":
    main()
