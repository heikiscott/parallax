"""
OpenAI official API provider implementation.

This provider uses official OpenAI Python SDK (openai.AsyncOpenAI).
"""

import os
import time
import asyncio
import random
import openai
from typing import Optional
from collections import deque

from .protocol import LLMProvider, LLMError
from core.observation.logger import get_logger

logger = get_logger(__name__)


class OpenAIProvider(LLMProvider):
    """
    Official OpenAI API provider using openai.AsyncOpenAI.

    This provider uses the official OpenAI Python SDK which includes:
    - Automatic retry with exponential backoff
    - Rate limiting handling
    - Better error handling and connection management
    """

    def __init__(
        self,
        model: str = "gpt-4o-mini",
        api_key: str | None = None,
        base_url: str | None = None,
        temperature: float = 0.3,
        max_tokens: int | None = 100 * 1024,
        enable_stats: bool = False,
        **kwargs,
    ):
        """
        Initialize OpenAI provider.

        Args:
            model: Model name (e.g., "gpt-4o-mini", "gpt-4o", "gpt-4-turbo")
            api_key: OpenAI API key (defaults to OPENAI_API_KEY env var)
            base_url: OpenAI base URL (defaults to official OpenAI endpoint)
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
            enable_stats: Enable usage statistics accumulation (default: False)
            **kwargs: Additional arguments (ignored for now)

        Environment Variables:
            OPENAI_TIMEOUT: API timeout in seconds (default: 30)
            OPENAI_SDK_MAX_RETRIES: SDK internal retries (default: 0)
            OPENAI_MAX_RETRIES: Application-level max retries (default: 5)
            OPENAI_BACKOFF_FACTOR: Backoff multiplier based on last request duration (default: 0.5)
            OPENAI_MIN_BACKOFF: Minimum backoff time in seconds (default: 5)
            OPENAI_MAX_BACKOFF: Maximum backoff time in seconds (default: 60)
            OPENAI_SLOW_THRESHOLD: Threshold to consider request as slow in seconds (default: 20)
            OPENAI_TIMEOUT_BACKOFF_MULTIPLIER: Multiplier for timeout-based extra backoff (default: 0.5)
            OPENAI_MAX_CONCURRENT_REQUESTS: Max concurrent requests (default: 50)
        """
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.enable_stats = enable_stats

        # Use OpenAI official API key and base URL
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.base_url = base_url or "https://api.openai.com/v1"

        # Read retry and concurrency config from environment
        self.timeout = float(os.getenv("OPENAI_TIMEOUT", "30"))
        self.sdk_max_retries = int(os.getenv("OPENAI_SDK_MAX_RETRIES", "0"))
        self.max_retries = int(os.getenv("OPENAI_MAX_RETRIES", "5"))

        # Adaptive backoff config: backoff based on last request duration
        self.backoff_factor = float(os.getenv("OPENAI_BACKOFF_FACTOR", "0.5"))
        self.min_backoff = float(os.getenv("OPENAI_MIN_BACKOFF", "5"))
        self.max_backoff = float(os.getenv("OPENAI_MAX_BACKOFF", "60"))
        self.slow_threshold = float(os.getenv("OPENAI_SLOW_THRESHOLD", "20"))
        self.timeout_backoff_multiplier = float(os.getenv("OPENAI_TIMEOUT_BACKOFF_MULTIPLIER", "0.5"))

        self.max_concurrent_requests = int(os.getenv("OPENAI_MAX_CONCURRENT_REQUESTS", "50"))

        # Initialize official OpenAI async client
        # We disable SDK's internal retry to have full control
        self.client = openai.AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=self.timeout,
            max_retries=self.sdk_max_retries,
        )

        # Concurrency control: limit simultaneous API calls to avoid overload
        self.semaphore = asyncio.Semaphore(self.max_concurrent_requests)

        # Optional statistics tracking
        if self.enable_stats:
            self.current_call_stats = None

    async def generate(
        self,
        prompt: str,
        temperature: float | None = None,
        max_tokens: int | None = None,
        extra_body: dict | None = None,
        response_format: dict | None = None,
    ) -> str:
        """
        Generate a response for the given prompt with retry and backoff.

        Args:
            prompt: Input prompt
            temperature: Override temperature for this request
            max_tokens: Override max tokens for this request
            extra_body: Extra parameters (not used with SDK)
            response_format: Response format specification

        Returns:
            Generated response text

        Raises:
            LLMError: If generation fails after all retries
        """
        # Call retry logic directly - semaphore is now inside the retry loop
        return await self._generate_with_retry(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            response_format=response_format,
        )

    async def _generate_with_retry(
        self,
        prompt: str,
        temperature: float | None = None,
        max_tokens: int | None = None,
        response_format: dict | None = None,
    ) -> str:
        """
        Internal method with exponential backoff logic.

        Exponential backoff strategy:
        - Each retry waits exponentially longer: base * (2 ^ attempt)
        - Prevents semaphore deadlock by releasing it during sleep
        - Adds jitter to prevent thundering herd
        """

        last_error = None
        last_duration = 0

        for attempt in range(self.max_retries):
            # Acquire semaphore only for the actual API call, not for sleep
            # This prevents deadlock where all semaphore slots are held by sleeping tasks
            async with self.semaphore:
                start_time = time.perf_counter()

                try:
                    # Convert prompt to messages format
                    messages = [{"role": "user", "content": prompt}]

                    # Prepare parameters
                    params = {
                        "model": self.model,
                        "messages": messages,
                        "temperature": temperature if temperature is not None else self.temperature,
                    }

                    # Add optional parameters
                    if max_tokens is not None:
                        params["max_tokens"] = int(max_tokens) if isinstance(max_tokens, str) else max_tokens
                    elif self.max_tokens is not None:
                        params["max_tokens"] = int(self.max_tokens) if isinstance(self.max_tokens, str) else self.max_tokens

                    if response_format is not None:
                        params["response_format"] = response_format

                    # Call OpenAI API
                    response = await self.client.chat.completions.create(**params)

                    end_time = time.perf_counter()
                    duration = end_time - start_time

                    # Extract result
                    content = response.choices[0].message.content
                    finish_reason = response.choices[0].finish_reason

                    # Log completion details
                    if finish_reason == 'stop':
                        logger.debug(f"[OpenAI-{self.model}] å®ŒæˆåŸå› : {finish_reason}")
                    else:
                        logger.warning(f"[OpenAI-{self.model}] å®ŒæˆåŸå› : {finish_reason}")

                    # Extract token usage
                    usage = response.usage
                    prompt_tokens = usage.prompt_tokens if usage else 0
                    completion_tokens = usage.completion_tokens if usage else 0
                    total_tokens = usage.total_tokens if usage else 0

                    # Log usage information
                    logger.debug(f"[OpenAI-{self.model}] APIè°ƒç”¨å®Œæˆ (attempt {attempt + 1})")
                    logger.debug(f"[OpenAI-{self.model}] è€—æ—¶: {duration:.2f}s")

                    logger.debug(f"[OpenAI-{self.model}] Prompt Tokens: {prompt_tokens:,}")
                    logger.debug(f"[OpenAI-{self.model}] Completion Tokens: {completion_tokens:,}")
                    logger.debug(f"[OpenAI-{self.model}] æ€»Tokenæ•°: {total_tokens:,}")

                    # Record statistics if enabled
                    if self.enable_stats:
                        self.current_call_stats = {
                            'prompt_tokens': prompt_tokens,
                            'completion_tokens': completion_tokens,
                            'total_tokens': total_tokens,
                            'duration': duration,
                            'timestamp': time.time(),
                        }

                    # Success - return immediately
                    return content

                except (openai.APIError, openai.APIConnectionError, openai.RateLimitError) as e:
                    # Retriable errors
                    error_time = time.perf_counter()
                    duration = error_time - start_time
                    last_error = e
                    last_duration = duration

                    error_type = type(e).__name__
                    logger.warning(f"[OpenAI-{self.model}] {error_type} (attempt {attempt + 1}/{self.max_retries})")
                    logger.warning(f"   â±ï¸  è€—æ—¶: {duration:.2f}s")
                    logger.warning(f"   ğŸ’¬ é”™è¯¯: {str(e)}")

                except Exception as e:
                    # Non-retriable errors - fail immediately
                    error_time = time.perf_counter()
                    duration = error_time - start_time
                    logger.error(f"[OpenAI-{self.model}] Unexpected error: {e}")
                    logger.error(f"   â±ï¸  è€—æ—¶: {duration:.2f}s")
                    logger.error(f"   ğŸ’¬ é”™è¯¯ä¿¡æ¯: {str(e)}")
                    raise LLMError(f"Request failed: {str(e)}")

            # Backoff logic outside semaphore - this prevents deadlock
            # If not the last attempt, apply comprehensive backoff strategy
            if last_error and attempt < self.max_retries - 1:
                # ç»¼åˆé€€é¿ç­–ç•¥ï¼š
                # 1. åŸºäºé‡è¯•æ¬¡æ•°çš„æŒ‡æ•°é€€é¿: min_backoff * (2 ^ attempt)
                # 2. åŸºäºè¯·æ±‚è€—æ—¶çš„é¢å¤–é€€é¿: è¶…æ—¶è¶Šé•¿ï¼Œé¢å¤–ç­‰å¾…è¶Šå¤š
                # 3. éšæœºæŠ–åŠ¨é¿å…é›ªå´©: åœ¨èŒƒå›´å†…éšæœºï¼Œé˜²æ­¢æ‰€æœ‰è¯·æ±‚åŒæ—¶é‡è¯•

                # 1. åŸºäºé‡è¯•æ¬¡æ•°çš„æŒ‡æ•°é€€é¿
                retry_backoff = self.min_backoff * (2 ** attempt)

                # 2. åŸºäºè¶…æ—¶çš„é¢å¤–é€€é¿: å¦‚æœè¯·æ±‚è€—æ—¶è¶…è¿‡é˜ˆå€¼ï¼Œé¢å¤–ç­‰å¾…
                timeout_backoff = 0
                if last_duration > self.slow_threshold:
                    # è¶…æ—¶è¶Šé•¿ï¼Œé¢å¤–ç­‰å¾…è¶Šå¤š (å¯é…ç½®å€æ•°ï¼Œé»˜è®¤50%)
                    timeout_backoff = (last_duration - self.slow_threshold) * self.timeout_backoff_multiplier
                    logger.warning(f"[OpenAI-{self.model}] æ£€æµ‹åˆ°æ…¢è¯·æ±‚/è¶…æ—¶ ({last_duration:.1f}s)ï¼Œå¢åŠ é€€é¿ {timeout_backoff:.1f}s")

                # ç»¼åˆé€€é¿æ—¶é—´
                total_backoff = retry_backoff + timeout_backoff
                total_backoff = min(total_backoff, self.max_backoff)

                # 3. éšæœºæŠ–åŠ¨ (Â±30%) é¿å…é›ªå´©
                jitter_range = total_backoff * 0.3
                jitter = random.uniform(-jitter_range, jitter_range)
                final_backoff = max(0, total_backoff + jitter)

                logger.info(f"[OpenAI-{self.model}] ç»¼åˆé€€é¿ {final_backoff:.1f}s")
                logger.info(f"   é‡è¯•é€€é¿: {retry_backoff:.1f}s [2^{attempt}]")
                if timeout_backoff > 0:
                    logger.info(f"   è¶…æ—¶é€€é¿: +{timeout_backoff:.1f}s (è¯·æ±‚è€—æ—¶ {last_duration:.1f}s)")
                logger.info(f"   éšæœºæŠ–åŠ¨: {jitter:+.1f}s")
                logger.info(f"   ä¸‹æ¬¡å°è¯•: attempt {attempt + 2}/{self.max_retries}")

                await asyncio.sleep(final_backoff)
            elif last_error:
                # Last attempt failed
                logger.error(f"[OpenAI-{self.model}] æ‰€æœ‰ {self.max_retries} æ¬¡é‡è¯•å‡å¤±è´¥")
                logger.error(f"   â±ï¸  æœ€åè€—æ—¶: {last_duration:.2f}s")
                logger.error(f"   ğŸ’¬ æœ€åé”™è¯¯: {str(last_error)}")
                raise LLMError(f"OpenAI API error after {self.max_retries} retries: {str(last_error)}")

        # This should never be reached due to the raise in the loop
        raise LLMError(f"Request failed after {self.max_retries} retries: {str(last_error)}")

    async def test_connection(self) -> bool:
        """
        Test the connection to the OpenAI API.

        Returns:
            True if connection successful, False otherwise
        """
        try:
            logger.info(f"ğŸ”— [OpenAI-{self.model}] æµ‹è¯•APIè¿æ¥...")
            test_response = await self.generate("Hello", temperature=0.1)
            success = len(test_response) > 0
            if success:
                logger.info(f"âœ… [OpenAI-{self.model}] APIè¿æ¥æµ‹è¯•æˆåŠŸ")
            else:
                logger.error(f"âŒ [OpenAI-{self.model}] APIè¿æ¥æµ‹è¯•å¤±è´¥: ç©ºå“åº”")
            return success
        except Exception as e:
            logger.error(f"âŒ [OpenAI-{self.model}] APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

    def get_current_call_stats(self) -> Optional[dict]:
        """Get statistics for the current call (if enabled)."""
        if self.enable_stats:
            return self.current_call_stats
        return None

    def __repr__(self) -> str:
        """String representation of the provider."""
        return f"OpenAIProvider(model={self.model}, base_url={self.base_url}, sdk=official)"
