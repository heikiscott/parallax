"""
Redisæ¶ˆæ¯åˆ†ç»„é˜Ÿåˆ—ç®¡ç†å™¨

åŸºäºRediså®ç°çš„å›ºå®šåˆ†åŒºé˜Ÿåˆ—ç®¡ç†å™¨ã€‚
æ ¸å¿ƒç‰¹æ€§ï¼š
1. å›ºå®š50ä¸ªåˆ†åŒºï¼Œç¼–å·001-050ï¼Œä¸å¯é…ç½®
2. group_keyé€šè¿‡hashè·¯ç”±åˆ°å›ºå®šåˆ†åŒº
3. æ”¯æŒå¤šé˜Ÿåˆ—å¹¶å‘æ¶ˆè´¹ï¼ŒåŸºäºowneræœºåˆ¶é˜²å†²çª
4. ä½¿ç”¨Redisçš„æœ‰åºé›†åˆ(ZSET)å­˜å‚¨æ¶ˆæ¯ï¼Œæ”¯æŒæŒ‰åˆ†æ•°æ’åºå’Œæ—¶é—´è¿‡æ»¤

âš ï¸ è­¦å‘Šï¼šåˆ†åŒºæ•°é‡å›ºå®šä¸º50ï¼Œä¿®æ”¹æ­¤é…ç½®ä¼šå¯¼è‡´ä¸¥é‡çš„æ•°æ®è·¯ç”±é”™è¯¯å’Œæ¶ˆæ¯ä¸¢å¤±ï¼
"""

import asyncio
import time
import random
import hashlib
from typing import Any, Dict, List, Optional, Tuple, Callable, Type
from dataclasses import dataclass, field
from enum import Enum

import redis.asyncio as redis

from core.observation.logger import get_logger
from common_utils.datetime_utils import get_now_with_timezone, to_iso_format
from core.queue.redis_group_queue.redis_group_queue_item import SimpleQueueItem
from core.queue.redis_group_queue.redis_group_queue_item import (
    RedisGroupQueueItem,
    SerializationMode,
)
from core.queue.redis_group_queue.redis_group_queue_lua_scripts import (
    ENQUEUE_SCRIPT,
    GET_QUEUE_STATS_SCRIPT,
    GET_ALL_PARTITIONS_STATS_SCRIPT,
    REBALANCE_PARTITIONS_SCRIPT,
    JOIN_CONSUMER_SCRIPT,
    EXIT_CONSUMER_SCRIPT,
    KEEPALIVE_CONSUMER_SCRIPT,
    CLEANUP_INACTIVE_OWNERS_SCRIPT,
    FORCE_CLEANUP_SCRIPT,
    GET_MESSAGES_SCRIPT,
)
from core.rate_limit.rate_limiter import rate_limit

logger = get_logger(__name__)


class ShutdownMode(Enum):
    """å…³é—­æ¨¡å¼æšä¸¾"""

    SOFT = "soft"  # è½¯æ€§å…³é—­ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ¶ˆæ¯ï¼Œæœ‰å»¶è¿Ÿæ—¶é—´æ§åˆ¶
    HARD = "hard"  # ç¡¬æ€§å…³é—­ï¼šç›´æ¥å…³é—­ï¼Œè®°å½•æœªå¤„ç†æ¶ˆæ¯æ•°


class ManagerState(Enum):
    """ç®¡ç†å™¨çŠ¶æ€æšä¸¾"""

    CREATED = "created"  # å·²åˆ›å»ºï¼Œæœªå¯åŠ¨
    STARTED = "started"  # å·²å¯åŠ¨ï¼Œæ­£åœ¨è¿è¡Œ
    SHUTDOWN = "shutdown"  # å·²å…³é—­ï¼Œä¸å¯å†å¯åŠ¨


@dataclass
class RedisPartitionStats:
    """Redisåˆ†åŒºç»Ÿè®¡ä¿¡æ¯"""

    partition: str
    current_size: int
    min_score: int
    max_score: int

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "partition": self.partition,
            "current_size": self.current_size,
            "min_score": self.min_score,
            "max_score": self.max_score,
        }


@dataclass
class RedisQueueStats:
    """Redisé˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯"""

    queue_name: str
    current_size: int
    last_activity_time: float
    min_score: int
    max_score: int
    total_delivered: int = 0
    total_consumed: int = 0
    last_deliver_time: Optional[str] = None
    last_consume_time: Optional[str] = None
    partitions: Optional[List[RedisPartitionStats]] = None

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        result = {
            "queue_name": self.queue_name,
            "current_size": self.current_size,
            "last_activity_time": self.last_activity_time,
            "min_score": self.min_score,
            "max_score": self.max_score,
            "total_delivered": self.total_delivered,
            "total_consumed": self.total_consumed,
            "last_deliver_time": self.last_deliver_time,
            "last_consume_time": self.last_consume_time,
        }
        if self.partitions:
            result["partitions"] = [p.to_dict() for p in self.partitions]
        return result


@dataclass
class RedisManagerStats:
    """Redisç®¡ç†å™¨æ•´ä½“ç»Ÿè®¡ä¿¡æ¯"""

    total_queues: int
    total_current_messages: int
    total_delivered_messages: int = 0
    total_consumed_messages: int = 0
    total_rejected_messages: int = 0
    start_time: str = field(
        default_factory=lambda: to_iso_format(get_now_with_timezone())
    )
    uptime_seconds: float = 0

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "total_queues": self.total_queues,
            "total_current_messages": self.total_current_messages,
            "total_delivered_messages": self.total_delivered_messages,
            "total_consumed_messages": self.total_consumed_messages,
            "total_rejected_messages": self.total_rejected_messages,
            "start_time": self.start_time,
            "uptime_seconds": self.uptime_seconds,
        }


class RedisGroupQueueManager:
    """
    Redisæ¶ˆæ¯åˆ†ç»„é˜Ÿåˆ—ç®¡ç†å™¨ï¼ˆåŠ¨æ€ownerç®¡ç†ç‰ˆæœ¬ï¼‰

    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. åŸºäºowner_activate_time_zsetç®¡ç†æ¶ˆè´¹è€…æ´»è·ƒçŠ¶æ€
    2. æ¯ä¸ªowneræ‹¥æœ‰ç‹¬ç«‹çš„queue_listï¼Œè®°å½•åˆ†é…çš„åˆ†åŒº
    3. æ”¯æŒåŠ¨æ€rebalanceï¼Œè‡ªåŠ¨åˆ†é…åˆ†åŒºç»™æ´»è·ƒæ¶ˆè´¹è€…
    4. æ¶ˆè´¹è€…åŠ å…¥/é€€å‡ºè‡ªåŠ¨è§¦å‘rebalance
    5. å®šæœŸæ¸…ç†ä¸æ´»è·ƒæ¶ˆè´¹è€…ï¼ˆé»˜è®¤5åˆ†é’Ÿä¸æ´»è·ƒï¼‰
    6. æ¶ˆè´¹è€…ä¿æ´»æœºåˆ¶ï¼ˆå»ºè®®æ¯30ç§’è°ƒç”¨ï¼‰
    7. æ”¯æŒå¼ºåˆ¶æ¸…ç†å’Œé‡ç½®
    8. æ¶ˆè´¹æ¶ˆæ¯æ—¶æ£€æŸ¥scoreå·®å€¼é˜ˆå€¼
    9. æ‰€æœ‰æ“ä½œé€šè¿‡Luaè„šæœ¬ä¿è¯åŸå­æ€§
    """

    # å›ºå®šåˆ†åŒºæ•°é‡ï¼Œå¯é…ç½®ä½†å»ºè®®ä¿æŒ50
    FIXED_PARTITION_COUNT = 50

    def __init__(
        self,
        redis_client: redis.Redis,
        key_prefix: str = "default",
        serialization_mode: SerializationMode = SerializationMode.JSON,
        item_class: Type[RedisGroupQueueItem] = None,
        sort_key_func: Optional[Callable[[RedisGroupQueueItem], int]] = None,
        max_total_messages: int = 20000,  # 2w
        queue_expire_seconds: int = 24 * 3600,  # 1å¤©
        activity_expire_seconds: int = 24 * 3600,  # 1å¤©
        enable_metrics: bool = True,
        log_interval_seconds: int = 600,  # 10åˆ†é’Ÿ
        owner_expire_seconds: int = 3600,  # ownerè¿‡æœŸæ—¶é—´ï¼Œé»˜è®¤1å°æ—¶
        inactive_threshold_seconds: int = 300,  # ä¸æ´»è·ƒé˜ˆå€¼ï¼Œé»˜è®¤5åˆ†é’Ÿ
        cleanup_interval_seconds: int = 300,  # å®šæœŸæ¸…ç†é—´éš”ï¼Œé»˜è®¤5åˆ†é’Ÿ
    ):
        """
        åˆå§‹åŒ–Redisæ¶ˆæ¯åˆ†ç»„é˜Ÿåˆ—ç®¡ç†å™¨

        Args:
            redis_client: Rediså®¢æˆ·ç«¯
            key_prefix: Redisé”®å‰ç¼€ï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„ç®¡ç†å™¨å®ä¾‹
            serialization_mode: åºåˆ—åŒ–æ¨¡å¼ï¼ˆJSONæˆ–BSONï¼‰
            item_class: é˜Ÿåˆ—é¡¹ç±»å‹ï¼Œå¿…é¡»ç»§æ‰¿è‡ªRedisGroupQueueItemï¼Œé»˜è®¤ä½¿ç”¨SimpleQueueItem
            sort_key_func: æ’åºé”®ç”Ÿæˆå‡½æ•°ï¼Œæ¥æ”¶RedisGroupQueueItemè¿”å›intåˆ†æ•°
            max_total_messages: æœ€å¤§æ€»æ¶ˆæ¯æ•°é‡é™åˆ¶
            queue_expire_seconds: é˜Ÿåˆ—è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            activity_expire_seconds: æ´»åŠ¨è®°å½•è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            enable_metrics: æ˜¯å¦å¯ç”¨ç»Ÿè®¡åŠŸèƒ½
            log_interval_seconds: æ—¥å¿—æ‰“å°é—´éš”ï¼ˆç§’ï¼‰
            owner_expire_seconds: ownerè¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤1å°æ—¶ï¼‰
            inactive_threshold_seconds: ä¸æ´»è·ƒé˜ˆå€¼ï¼ˆç§’ï¼Œé»˜è®¤5åˆ†é’Ÿï¼‰
            cleanup_interval_seconds: å®šæœŸæ¸…ç†é—´éš”ï¼ˆç§’ï¼Œé»˜è®¤5åˆ†é’Ÿï¼‰
        """
        self.redis_client = redis_client
        self.key_prefix = key_prefix
        self.serialization_mode = serialization_mode
        # è®¾ç½®é»˜è®¤çš„item_classä¸ºSimpleQueueItem
        if item_class is None:
            self.item_class = SimpleQueueItem
        else:
            self.item_class = item_class
        self.sort_key_func = sort_key_func or self._default_sort_key
        self.max_total_messages = max_total_messages
        self.queue_expire_seconds = queue_expire_seconds
        self.activity_expire_seconds = activity_expire_seconds
        self.enable_metrics = enable_metrics
        self.log_interval_seconds = log_interval_seconds
        self.owner_expire_seconds = owner_expire_seconds
        self.inactive_threshold_seconds = inactive_threshold_seconds
        self.cleanup_interval_seconds = cleanup_interval_seconds

        # Redisé”®æ¨¡å¼ - æ–°çš„åŠ¨æ€ownerç®¡ç†æ¨¡å¼
        self.queue_prefix = f"{key_prefix}:queue:"  # é˜Ÿåˆ—é”®å‰ç¼€ï¼Œç”¨äºLuaè„šæœ¬
        self.queue_key_pattern = (
            f"{key_prefix}:queue:{{partition}}"  # partitionä¸º001-050
        )
        self.owner_activate_time_zset_key = (
            f"{key_prefix}:owner_activate_time_zset"  # owneræ´»è·ƒæ—¶é—´zset
        )
        self.queue_list_prefix = f"{key_prefix}:queue_list:"  # ownerçš„queue_listå‰ç¼€
        self.counter_key = f"{key_prefix}:counter"

        # è¿›ç¨‹çº§åˆ«çš„owner IDï¼ˆå¯åŠ¨æ—¶ç”Ÿæˆï¼Œå…¨å±€å”¯ä¸€ï¼‰
        self.owner_id = (
            f"{self.key_prefix}_{int(time.time())}_{random.randint(10000, 99999)}"
        )

        # ç»´æŠ¤owneræœ€åkeepaliveæ—¶é—´æˆ³mappingï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰
        self.owner_last_keepalive_time = {}

        # ç”Ÿæˆå›ºå®šåˆ†åŒºåç§°åˆ—è¡¨ï¼š001, 002, ..., 050
        self.partition_names = [
            f"{i:03d}" for i in range(1, self.FIXED_PARTITION_COUNT + 1)
        ]

        # ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯
        self._manager_stats = RedisManagerStats(
            total_queues=0, total_current_messages=0
        )

        # å¼‚æ­¥é”ï¼Œä¿æŠ¤ç»Ÿè®¡ä¿¡æ¯
        self._stats_lock = asyncio.Lock()

        # å¯åŠ¨æ—¶é—´
        self._start_time = time.time()

        # å®šæœŸä»»åŠ¡
        self._log_task: Optional[asyncio.Task] = None
        self._cleanup_task: Optional[asyncio.Task] = None
        self._running = False

        # ç®¡ç†å™¨çŠ¶æ€
        self._state = ManagerState.CREATED

        # é¢„ç¼–è¯‘Luaè„šæœ¬
        self._enqueue_script = None
        self._get_stats_script = None
        self._get_all_partitions_stats_script = None
        self._rebalance_partitions_script = None
        self._join_consumer_script = None
        self._exit_consumer_script = None
        self._keepalive_consumer_script = None
        self._cleanup_inactive_owners_script = None
        self._force_cleanup_script = None
        self._get_messages_script = None

        logger.info(
            "ğŸš€ RedisGroupQueueManager[%s] åˆå§‹åŒ–å®Œæˆ: key_prefix=%s, max_total_messages=%d",
            self.key_prefix,
            self.key_prefix,
            self.max_total_messages,
        )

    def _default_sort_key(self, _item: RedisGroupQueueItem) -> int:
        """
        é»˜è®¤æ’åºé”®ç”Ÿæˆå‡½æ•°ï¼šä½¿ç”¨å½“å‰æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰

        Args:
            item: é˜Ÿåˆ—é¡¹

        Returns:
            int: æ’åºåˆ†æ•°ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰
        """
        return int(time.time() * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’æ•´æ•°

    def _hash_group_key_to_partition(self, group_key: str) -> str:
        """
        å°†group_keyé€šè¿‡hashè·¯ç”±åˆ°å›ºå®šåˆ†åŒº

        Args:
            group_key: åˆ†ç»„é”®

        Returns:
            str: åˆ†åŒºåç§°ï¼ˆ001-100ï¼‰
        """
        # ä½¿ç”¨MD5 hashç¡®ä¿åˆ†å¸ƒå‡åŒ€
        hash_value = hashlib.md5(group_key.encode('utf-8')).hexdigest()
        # å–å‰8ä½è½¬ä¸ºæ•´æ•°ï¼Œå†å–æ¨¡
        partition_index = int(hash_value[:8], 16) % self.FIXED_PARTITION_COUNT
        return self.partition_names[partition_index]

    def _get_queue_key(self, partition: str) -> str:
        """è·å–é˜Ÿåˆ—Redisé”®"""
        return self.queue_key_pattern.format(partition=partition)

    def _get_queue_list_key(self, owner_id: Optional[str] = None) -> str:
        """è·å–ownerçš„queue_list Redisé”®"""
        if owner_id is None:
            owner_id = self.owner_id
        return f"{self.queue_list_prefix}{owner_id}"

    def _parse_rebalance_result(
        self, result: Any, expected_count: int
    ) -> Tuple[bool, Tuple]:
        """
        è§£ærebalanceç›¸å…³è„šæœ¬çš„è¿”å›ç»“æœ

        Args:
            result: Luaè„šæœ¬è¿”å›çš„ç»“æœ
            expected_count: æœŸæœ›çš„è¿”å›å€¼æ•°é‡ (2 for rebalance/join/exit, 3 for cleanup)

        Returns:
            Tuple[bool, Tuple]: (æ˜¯å¦æˆåŠŸ, è§£æåçš„ç»“æœ)
        """
        # æ£€æŸ¥è¿”å›ç»“æœæ ¼å¼
        if not isinstance(result, (list, tuple)) or len(result) < expected_count:
            logger.error(
                "âŒ RedisGroupQueueManager[%s] è„šæœ¬è¿”å›æ ¼å¼é”™è¯¯: æœŸæœ›%dä¸ªå€¼ï¼Œå®é™…å¾—åˆ°%s",
                self.key_prefix,
                expected_count,
                result,
            )
            return False, tuple([0] * expected_count)

        # æå–åŸºæœ¬å€¼
        if expected_count == 2:
            owner_count, assigned_partitions_flat = result
            parsed_result = (
                owner_count,
                self._convert_flat_to_dict(assigned_partitions_flat),
            )
        elif expected_count == 3:
            cleaned_count, owner_count, assigned_partitions_flat = result
            parsed_result = (
                cleaned_count,
                owner_count,
                self._convert_flat_to_dict(assigned_partitions_flat),
            )
        else:
            return False, tuple([0] * expected_count)

        return True, parsed_result

    def _convert_flat_to_dict(
        self, assigned_partitions_flat: Any
    ) -> Dict[str, List[str]]:
        """
        å°†æ‰å¹³æ•°ç»„è½¬æ¢ä¸ºå­—å…¸æ ¼å¼

        Args:
            assigned_partitions_flat: æ‰å¹³æ•°ç»„ [owner_id1, [partitions1], owner_id2, [partitions2], ...]

        Returns:
            Dict[str, List[str]]: åˆ†é…ç»“æœå­—å…¸
        """
        assigned_partitions = {}
        if (
            isinstance(assigned_partitions_flat, list)
            and len(assigned_partitions_flat) > 0
        ):
            for i in range(0, len(assigned_partitions_flat), 2):
                if i + 1 < len(assigned_partitions_flat):
                    owner_id = self._safe_decode_redis_value(
                        assigned_partitions_flat[i]
                    )
                    partitions_raw = assigned_partitions_flat[i + 1]
                    # å¤„ç†åˆ†åŒºåˆ—è¡¨ï¼Œæ¯ä¸ªåˆ†åŒºåä¹Ÿéœ€è¦è§£ç 
                    if isinstance(partitions_raw, list):
                        partitions = [
                            self._safe_decode_redis_value(p) for p in partitions_raw
                        ]
                    else:
                        partitions = [self._safe_decode_redis_value(partitions_raw)]
                    assigned_partitions[owner_id] = partitions
        return assigned_partitions

    def _safe_decode_redis_value(self, value: Any) -> str:
        """
        å®‰å…¨è§£ç Redisè¿”å›å€¼ï¼Œå…¼å®¹byteså’Œstrç±»å‹

        å½“Rediså®¢æˆ·ç«¯ä½¿ç”¨decode_responses=Falseæ—¶ï¼Œè¿”å›å€¼ä¸ºbytesç±»å‹
        å½“Rediså®¢æˆ·ç«¯ä½¿ç”¨decode_responses=Trueæ—¶ï¼Œè¿”å›å€¼ä¸ºstrç±»å‹

        Args:
            value: Redisè¿”å›çš„å€¼ï¼Œå¯èƒ½æ˜¯bytesæˆ–str

        Returns:
            str: è§£ç åçš„å­—ç¬¦ä¸²
        """
        if isinstance(value, bytes):
            return value.decode('utf-8')
        elif isinstance(value, str):
            return value
        else:
            return str(value)

    async def _check_and_keepalive_if_needed(self, owner_id: str) -> bool:
        """
        æ£€æŸ¥å¹¶æŒ‰éœ€æ‰§è¡Œkeepalive

        æ£€æŸ¥ownerä¸Šæ¬¡keepaliveæ—¶é—´ï¼Œå¦‚æœä¸å­˜åœ¨è®°å½•æˆ–è¶…è¿‡30ç§’ï¼Œåˆ™è§¦å‘ä¸€æ¬¡keepaliveã€‚

        Args:
            owner_id: æ¶ˆè´¹è€…ID

        Returns:
            bool: æ˜¯å¦æ‰§è¡Œäº†keepaliveæ“ä½œ
        """
        current_time_ms = int(time.time() * 1000)
        last_keepalive_time = self.owner_last_keepalive_time.get(owner_id, 0)

        # å¦‚æœä¸å­˜åœ¨è®°å½•æˆ–è€…è¶…è¿‡30ç§’ï¼Œè§¦å‘keepalive
        if (
            last_keepalive_time == 0 or (current_time_ms - last_keepalive_time) > 30000
        ):  # 30ç§’ = 30000æ¯«ç§’
            logger.debug(
                "ğŸ’“ RedisGroupQueueManager[%s] æŒ‰éœ€è§¦å‘keepalive: owner_id=%s, è·ç¦»ä¸Šæ¬¡=%.1fç§’",
                self.key_prefix,
                owner_id,
                (current_time_ms - last_keepalive_time) / 1000.0,
            )
            # è§¦å‘keepaliveå¹¶æ›´æ–°æ—¶é—´æˆ³
            try:
                success = await self.keepalive_consumer(owner_id)
                if success:
                    self.owner_last_keepalive_time[owner_id] = current_time_ms
                    return True
                else:
                    logger.warning(
                        "âš ï¸ RedisGroupQueueManager[%s] æŒ‰éœ€keepaliveå¤±è´¥: owner_id=%s, keepalive_consumerè¿”å›False",
                        self.key_prefix,
                        owner_id,
                    )
                    return False
            except (redis.RedisError, ValueError, TypeError) as e:
                logger.warning(
                    "âš ï¸ RedisGroupQueueManager[%s] æŒ‰éœ€keepaliveå¼‚å¸¸: owner_id=%s, é”™è¯¯=%s",
                    self.key_prefix,
                    owner_id,
                    e,
                )
                return False
        else:
            logger.debug(
                "ğŸ’“ RedisGroupQueueManager[%s] æ— éœ€keepalive: owner_id=%s, è·ç¦»ä¸Šæ¬¡=%.1fç§’",
                self.key_prefix,
                owner_id,
                (current_time_ms - last_keepalive_time) / 1000.0,
            )
            return False

    async def _ensure_scripts_loaded(self):
        """ç¡®ä¿Luaè„šæœ¬å·²åŠ è½½"""
        if self._enqueue_script is None:
            self._enqueue_script = self.redis_client.register_script(ENQUEUE_SCRIPT)
            self._get_stats_script = self.redis_client.register_script(
                GET_QUEUE_STATS_SCRIPT
            )
            self._get_all_partitions_stats_script = self.redis_client.register_script(
                GET_ALL_PARTITIONS_STATS_SCRIPT
            )
            self._rebalance_partitions_script = self.redis_client.register_script(
                REBALANCE_PARTITIONS_SCRIPT
            )
            self._join_consumer_script = self.redis_client.register_script(
                JOIN_CONSUMER_SCRIPT
            )
            self._exit_consumer_script = self.redis_client.register_script(
                EXIT_CONSUMER_SCRIPT
            )
            self._keepalive_consumer_script = self.redis_client.register_script(
                KEEPALIVE_CONSUMER_SCRIPT
            )
            self._cleanup_inactive_owners_script = self.redis_client.register_script(
                CLEANUP_INACTIVE_OWNERS_SCRIPT
            )
            self._force_cleanup_script = self.redis_client.register_script(
                FORCE_CLEANUP_SCRIPT
            )
            self._get_messages_script = self.redis_client.register_script(
                GET_MESSAGES_SCRIPT
            )

    @rate_limit(max_rate=200, time_period=1)
    async def deliver_message(
        self,
        group_key: str,
        item: RedisGroupQueueItem,
        return_mode: str = "normal",
        max_total_messages: int = None,
    ) -> bool:
        """
        æŠ•é€’æ¶ˆæ¯åˆ°æŒ‡å®šåˆ†ç»„é˜Ÿåˆ—

        Args:
            group_key: åˆ†ç»„é”®ï¼Œé€šè¿‡hashè·¯ç”±åˆ°å›ºå®šåˆ†åŒº
            item: æ¶ˆæ¯æ•°æ®é¡¹ï¼Œå¿…é¡»å®ç°RedisGroupQueueItemæ¥å£
            return_mode: è¿”å›æ¨¡å¼ï¼Œnormalåªè¿”å›boolï¼Œreject_reasonä¹Ÿè¿”å›æ‹’ç»åŸå› 
        Returns:
            bool: æŠ•é€’æ˜¯å¦æˆåŠŸ
        """
        try:
            await self._ensure_scripts_loaded()

            # é€šè¿‡hashè·¯ç”±åˆ°å›ºå®šåˆ†åŒº
            partition = self._hash_group_key_to_partition(group_key)

            # ç”Ÿæˆæ’åºåˆ†æ•°
            sort_score = self.sort_key_func(item)

            # æ ¹æ®åºåˆ—åŒ–æ¨¡å¼åºåˆ—åŒ–æ¶ˆæ¯
            if self.serialization_mode == SerializationMode.BSON:
                message_data = item.to_bson_bytes()
            else:  # JSONæ¨¡å¼
                message_data = item.to_json_str()

            # è·å–é˜Ÿåˆ—é”®
            queue_key = self._get_queue_key(partition)

            # æ‰§è¡ŒLuaè„šæœ¬æŠ•é€’æ¶ˆæ¯
            result = await self._enqueue_script(
                keys=[queue_key, self.counter_key],
                args=[
                    message_data,
                    sort_score,
                    self.queue_expire_seconds,
                    self.activity_expire_seconds,
                    (
                        max_total_messages
                        if max_total_messages is not None
                        else self.max_total_messages
                    ),
                ],
            )

            success, new_count, message = result

            # å®‰å…¨è§£ç æ¶ˆæ¯å†…å®¹ï¼Œå…¼å®¹byteså’Œstrç±»å‹
            message_str = self._safe_decode_redis_value(message)

            if success:
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                async with self._stats_lock:
                    self._manager_stats.total_delivered_messages += 1
                    self._manager_stats.total_current_messages = new_count

                logger.debug(
                    "âœ… RedisGroupQueueManager[%s] æ¶ˆæ¯æŠ•é€’æˆåŠŸ: group_key=%s->partition=%s, score=%.3f, æ€»ç•™å­˜=%d",
                    self.key_prefix,
                    group_key,
                    partition,
                    sort_score,
                    new_count,
                )
                if return_mode == "normal":
                    return True
                else:
                    return True, message_str
            else:
                # æŠ•é€’å¤±è´¥
                async with self._stats_lock:
                    self._manager_stats.total_rejected_messages += 1

                logger.warning(
                    "âŒ RedisGroupQueueManager[%s] æŠ•é€’è¢«æ‹’ç»: group_key=%s->partition=%s, åŸå› =%s",
                    self.key_prefix,
                    group_key,
                    partition,
                    message_str,
                )
                if return_mode == "normal":
                    return False
                else:
                    return False, message_str

        except (redis.RedisError, ValueError, TypeError) as e:
            # æ³¨æ„ï¼šè¿™é‡Œpartitionå¯èƒ½æœªå®šä¹‰ï¼Œéœ€è¦å®‰å…¨å¤„ç†
            try:
                partition = self._hash_group_key_to_partition(group_key)
                logger.error(
                    "âŒ RedisGroupQueueManager[%s] æŠ•é€’æ¶ˆæ¯å¤±è´¥: group_key=%s->partition=%s, é”™è¯¯=%s",
                    self.key_prefix,
                    group_key,
                    partition,
                    e,
                )
            except (redis.RedisError, ValueError, TypeError):
                logger.error(
                    "âŒ RedisGroupQueueManager[%s] æŠ•é€’æ¶ˆæ¯å¤±è´¥: group_key=%s, é”™è¯¯=%s",
                    self.key_prefix,
                    group_key,
                    e,
                )
            if return_mode == "normal":
                return False
            else:
                return False, "æŠ•é€’æŠ¥é”™"

    @rate_limit(
        max_rate=4, time_period=1, key_func=lambda owner_id: f"get_messages_{owner_id}"
    )
    async def get_messages(
        self,
        score_threshold: int,
        current_score: Optional[int] = None,
        owner_id: Optional[str] = None,
        _retry_depth: int = 2,
    ) -> List[RedisGroupQueueItem]:
        """
        è·å–æ¶ˆæ¯

        éå†æ‰€æœ‰åˆ†é…ç»™è¯¥ownerçš„åˆ†åŒºï¼Œæ¯ä¸ªåˆ†åŒºå°è¯•è·å–1ä¸ªæ¶ˆæ¯ã€‚
        æŒ‰éœ€keepaliveæœºåˆ¶ï¼šæ£€æŸ¥ä¸Šæ¬¡keepaliveæ—¶é—´ï¼Œè¶…è¿‡30ç§’åˆ™è§¦å‘ä¸€æ¬¡keepaliveã€‚

        Args:
            score_threshold: scoreå·®å€¼é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰ï¼Œå¿…ä¼ å‚æ•°
            current_score: å½“å‰scoreï¼Œç”¨äºç©ºé˜Ÿåˆ—æ—¶çš„thresholdæ¯”è¾ƒï¼Œå¯é€‰å‚æ•°
            owner_id: æ¶ˆè´¹è€…IDï¼Œé»˜è®¤ä½¿ç”¨self.owner_id
            _retry_depth: å†…éƒ¨å‚æ•°ï¼Œé€’å½’é‡è¯•æ·±åº¦é™åˆ¶ï¼Œé˜²æ­¢æ— é™å¾ªç¯

        Returns:
            List[RedisGroupQueueItem]: æ¶ˆæ¯åˆ—è¡¨
        """
        try:
            await self._ensure_scripts_loaded()

            if owner_id is None:
                owner_id = self.owner_id

            # æŒ‰éœ€keepaliveæœºåˆ¶
            await self._check_and_keepalive_if_needed(owner_id)

            # æ‰§è¡Œè·å–æ¶ˆæ¯è„šæœ¬
            result = await self._get_messages_script(
                keys=[
                    self.owner_activate_time_zset_key,
                    self.queue_list_prefix,
                    self.queue_prefix,
                    self.counter_key,
                ],
                args=[
                    owner_id,
                    self.owner_expire_seconds,
                    score_threshold,
                    (
                        current_score
                        if current_score is not None
                        else self._default_sort_key(None)
                    ),
                ],
            )

            status, messages_data = result

            # å®‰å…¨è§£ç çŠ¶æ€å€¼ï¼Œå…¼å®¹byteså’Œstrç±»å‹
            status_str = self._safe_decode_redis_value(status)

            if status_str == "JOIN_REQUIRED":
                # æ£€æŸ¥é€’å½’æ·±åº¦ï¼Œé˜²æ­¢æ— é™å¾ªç¯
                if _retry_depth <= 0:
                    logger.error(
                        "âŒ RedisGroupQueueManager[%s] JOIN_REQUIREDé‡è¯•æ¬¡æ•°è€—å°½: owner_id=%s",
                        self.key_prefix,
                        owner_id,
                    )
                    raise RuntimeError(
                        f"JOIN_REQUIREDé‡è¯•æ¬¡æ•°è€—å°½: owner_id={owner_id}"
                    )

                logger.info(
                    "ğŸ”„ RedisGroupQueueManager[%s] éœ€è¦åŠ å…¥æ¶ˆè´¹è€…: owner_id=%s, å‰©ä½™é‡è¯•æ¬¡æ•°=%d",
                    self.key_prefix,
                    owner_id,
                    _retry_depth - 1,
                )
                # è‡ªåŠ¨åŠ å…¥æ¶ˆè´¹è€…
                await self.join_consumer(owner_id)
                # é‡æ–°è·å–æ¶ˆæ¯ï¼Œé€’å‡é‡è¯•æ·±åº¦
                return await self.get_messages(
                    score_threshold, current_score, owner_id, _retry_depth - 1
                )

            if status_str == "NO_QUEUES":
                logger.warning(
                    "ğŸ“­ RedisGroupQueueManager[%s] æ¶ˆè´¹è€…æ— åˆ†é…é˜Ÿåˆ—: owner_id=%s",
                    self.key_prefix,
                    owner_id,
                )
                return []

            # è§£ææ¶ˆæ¯æ•°æ®
            messages = []
            for message_data in messages_data:
                try:
                    # æ ¹æ®åºåˆ—åŒ–æ¨¡å¼ååºåˆ—åŒ–æ¶ˆæ¯
                    if self.serialization_mode == SerializationMode.BSON:
                        # BSON å­—èŠ‚æ•°æ®
                        item = self.item_class.from_bson_bytes(message_data)
                    else:
                        # JSON å­—ç¬¦ä¸²
                        item = self.item_class.from_json_str(message_data)
                    messages.append(item)
                except (redis.RedisError, ValueError, TypeError) as e:
                    logger.warning(
                        "âš ï¸ RedisGroupQueueManager[%s] æ¶ˆæ¯ååºåˆ—åŒ–å¤±è´¥: %s",
                        self.key_prefix,
                        e,
                    )

            if messages:
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                async with self._stats_lock:
                    self._manager_stats.total_consumed_messages += len(messages)

                logger.debug(
                    "ğŸ“¤ RedisGroupQueueManager[%s] è·å–æ¶ˆæ¯æˆåŠŸ: owner_id=%s, è·å–æ•°é‡=%d",
                    self.key_prefix,
                    owner_id,
                    len(messages),
                )
            else:
                logger.debug(
                    "ğŸ“­ RedisGroupQueueManager[%s] æ— å¯æ¶ˆè´¹æ¶ˆæ¯: owner_id=%s",
                    self.key_prefix,
                    owner_id,
                )

            return messages

        except (redis.RedisError, ValueError, TypeError) as e:
            logger.error(
                "âŒ RedisGroupQueueManager[%s] è·å–æ¶ˆæ¯å¤±è´¥: owner_id=%s, é”™è¯¯=%s",
                self.key_prefix,
                owner_id,
                e,
            )
            return []

    # ==================== æ–°çš„åŠ¨æ€ownerç®¡ç†æ–¹æ³• ====================

    @rate_limit(max_rate=1, time_period=1, key_func=lambda: "rebalance_partitions")
    async def rebalance_partitions(self) -> Tuple[int, Dict[str, List[str]]]:
        """
        Rebalanceé‡æ–°åˆ†åŒº

        åŸºäºowner_activate_time_zsetæ¸…ç†æ‰æ‰€æœ‰çš„ownerçš„queue_listï¼Œ
        å¹³å‡åˆ†é…ä¸€ä¸‹åˆ†åŒºï¼Œç»™æ¯ä¸ªownerä¸€ä¸ªæ–°çš„queue_listã€‚

        Returns:
            Tuple[int, Dict[str, List[str]]]: (owneræ•°é‡, åˆ†é…ç»“æœå­—å…¸)
        """
        try:
            await self._ensure_scripts_loaded()

            # æ‰§è¡Œrebalanceè„šæœ¬
            result = await self._rebalance_partitions_script(
                keys=[self.owner_activate_time_zset_key, self.queue_list_prefix],
                args=[self.FIXED_PARTITION_COUNT, self.owner_expire_seconds],
            )

            # è§£æè¿”å›ç»“æœ
            success, (owner_count, assigned_partitions) = self._parse_rebalance_result(
                result, 2
            )
            if not success:
                return 0, {}

            logger.info(
                "ğŸ”„ RedisGroupQueueManager[%s] Rebalanceåˆ†åŒºå®Œæˆ: owneræ•°é‡=%d, åˆ†åŒºåˆ†é…=%s",
                self.key_prefix,
                owner_count,
                assigned_partitions,
            )

            return owner_count, assigned_partitions

        except (redis.RedisError, ValueError, TypeError) as e:
            logger.error(
                "âŒ RedisGroupQueueManager[%s] Rebalanceåˆ†åŒºå¤±è´¥: é”™è¯¯=%s",
                self.key_prefix,
                e,
            )
            return 0, {}

    @rate_limit(
        max_rate=1, time_period=1, key_func=lambda owner_id: f"join_consumer_{owner_id}"
    )
    async def join_consumer(
        self, owner_id: Optional[str] = None
    ) -> Tuple[int, Dict[str, List[str]]]:
        """
        åŠ å…¥æ¶ˆè´¹è€…

        åŠ å…¥owner_activate_time_zsetï¼Œç„¶årebalanceé‡æ–°åˆ†åŒºã€‚

        Args:
            owner_id: æ¶ˆè´¹è€…IDï¼Œé»˜è®¤ä½¿ç”¨self.owner_id

        Returns:
            Tuple[int, Dict[str, List[str]]]: (owneræ•°é‡, åˆ†é…ç»“æœå­—å…¸)
        """
        try:
            await self._ensure_scripts_loaded()

            if owner_id is None:
                owner_id = self.owner_id

            current_time = int(time.time() * 1000)  # æ¯«ç§’æ—¶é—´æˆ³

            # æ‰§è¡ŒåŠ å…¥æ¶ˆè´¹è€…è„šæœ¬
            result = await self._join_consumer_script(
                keys=[self.owner_activate_time_zset_key, self.queue_list_prefix],
                args=[
                    owner_id,
                    current_time,
                    self.owner_expire_seconds,
                    self.FIXED_PARTITION_COUNT,
                ],
            )

            # è§£æè¿”å›ç»“æœ
            success, (owner_count, assigned_partitions) = self._parse_rebalance_result(
                result, 2
            )
            if not success:
                return 0, {}

            # åˆå§‹åŒ–ownerçš„keepaliveæ—¶é—´æˆ³
            current_time_ms = int(time.time() * 1000)
            self.owner_last_keepalive_time[owner_id] = current_time_ms

            logger.info(
                "âœ… RedisGroupQueueManager[%s] æ¶ˆè´¹è€…åŠ å…¥æˆåŠŸ: owner_id=%s, owneræ•°é‡=%d, åˆ†é…ç»“æœ=%s",
                self.key_prefix,
                owner_id,
                owner_count,
                assigned_partitions,
            )

            return owner_count, assigned_partitions

        except (redis.RedisError, ValueError, TypeError) as e:
            logger.error(
                "âŒ RedisGroupQueueManager[%s] æ¶ˆè´¹è€…åŠ å…¥å¤±è´¥: owner_id=%s, é”™è¯¯=%s",
                self.key_prefix,
                owner_id,
                e,
            )
            return 0, {}

    @rate_limit(
        max_rate=1, time_period=1, key_func=lambda owner_id: f"exit_consumer_{owner_id}"
    )
    async def exit_consumer(
        self, owner_id: Optional[str] = None
    ) -> Tuple[int, Dict[str, List[str]]]:
        """
        æ¶ˆè´¹è€…é€€å‡º

        ä»owner_activate_time_zsetåˆ é™¤ï¼Œç„¶årebalanceé‡æ–°åˆ†åŒºã€‚

        Args:
            owner_id: æ¶ˆè´¹è€…IDï¼Œé»˜è®¤ä½¿ç”¨self.owner_id

        Returns:
            Tuple[int, Dict[str, List[str]]]: (owneræ•°é‡, åˆ†é…ç»“æœå­—å…¸)
        """
        try:
            await self._ensure_scripts_loaded()

            if owner_id is None:
                owner_id = self.owner_id

            # æ‰§è¡Œæ¶ˆè´¹è€…é€€å‡ºè„šæœ¬
            result = await self._exit_consumer_script(
                keys=[self.owner_activate_time_zset_key, self.queue_list_prefix],
                args=[owner_id, self.owner_expire_seconds, self.FIXED_PARTITION_COUNT],
            )

            # è§£æè¿”å›ç»“æœ
            success, (owner_count, assigned_partitions) = self._parse_rebalance_result(
                result, 2
            )
            if not success:
                return 0, {}

            # å°†é€€å‡ºçš„æ¶ˆè´¹è€…ä»keepaliveæ—¶é—´æˆ³mappingä¸­ç§»é™¤
            self.owner_last_keepalive_time.pop(owner_id, None)

            logger.info(
                "ğŸ‘‹ RedisGroupQueueManager[%s] æ¶ˆè´¹è€…é€€å‡ºæˆåŠŸ: owner_id=%s, å‰©ä½™owneræ•°é‡=%d, é‡æ–°åˆ†é…ç»“æœ=%s",
                self.key_prefix,
                owner_id,
                owner_count,
                assigned_partitions,
            )

            return owner_count, assigned_partitions

        except (redis.RedisError, ValueError, TypeError) as e:
            logger.error(
                "âŒ RedisGroupQueueManager[%s] æ¶ˆè´¹è€…é€€å‡ºå¤±è´¥: owner_id=%s, é”™è¯¯=%s",
                self.key_prefix,
                owner_id,
                e,
            )
            return 0, {}

    @rate_limit(
        max_rate=1,
        time_period=2,
        key_func=lambda owner_id: f"keepalive_consumer_{owner_id}",
    )
    async def keepalive_consumer(self, owner_id: Optional[str] = None) -> bool:
        """
        æ¶ˆè´¹è€…ä¿æ´»

        æ¶ˆè´¹è€…å®šæ—¶æ›´æ–°owner_activate_time_zsetçš„æ—¶é—´ã€‚
        å»ºè®®æ¯30ç§’è°ƒç”¨ä¸€æ¬¡ã€‚

        Args:
            owner_id: æ¶ˆè´¹è€…ID

        Returns:
            bool: ä¿æ´»æ˜¯å¦æˆåŠŸ
        """
        try:
            await self._ensure_scripts_loaded()

            current_time = int(time.time() * 1000)  # æ¯«ç§’æ—¶é—´æˆ³

            # æ‰§è¡Œæ¶ˆè´¹è€…ä¿æ´»è„šæœ¬
            result = await self._keepalive_consumer_script(
                keys=[self.owner_activate_time_zset_key, self.queue_list_prefix],
                args=[owner_id, current_time, self.owner_expire_seconds],
            )

            success = bool(result)

            if success:
                logger.debug(
                    "ğŸ’“ RedisGroupQueueManager[%s] æ¶ˆè´¹è€…ä¿æ´»æˆåŠŸ: owner_id=%s",
                    self.key_prefix,
                    owner_id,
                )
            else:
                logger.warning(
                    "âš ï¸ RedisGroupQueueManager[%s] æ¶ˆè´¹è€…ä¿æ´»å¤±è´¥ï¼Œqueue_listä¸å­˜åœ¨: owner_id=%s",
                    self.key_prefix,
                    owner_id,
                )

            return success

        except (redis.RedisError, ValueError, TypeError) as e:
            logger.error(
                "âŒ RedisGroupQueueManager[%s] æ¶ˆè´¹è€…ä¿æ´»å¤±è´¥: owner_id=%s, é”™è¯¯=%s",
                self.key_prefix,
                owner_id,
                e,
            )
            return False

    @rate_limit(max_rate=1, time_period=5, key_func=lambda: "cleanup_inactive_owners")
    async def cleanup_inactive_owners(self) -> Tuple[int, int, Dict[str, List[str]]]:
        """
        å®šæœŸæ¸…ç†å’Œé‡ç½®

        éå†æ¸…ç†æ‰æ‰€æœ‰å·²ç»ä¸æ´»è·ƒçš„ownerï¼ˆæ¯”å¦‚è¯´5åˆ†é’Ÿæ²¡æœ‰æ´»è·ƒï¼‰ï¼Œ
        å¦‚æœæœ‰ä¸æ´»è·ƒçš„ï¼Œrebalanceé‡æ–°åˆ†åŒºã€‚

        Returns:
            Tuple[int, int, Dict[str, List[str]]]: (æ¸…ç†çš„owneræ•°é‡, å‰©ä½™owneræ•°é‡, é‡æ–°åˆ†é…ç»“æœ)
        """
        try:
            await self._ensure_scripts_loaded()

            current_time = int(time.time() * 1000)  # æ¯«ç§’æ—¶é—´æˆ³
            inactive_threshold = current_time - (
                self.inactive_threshold_seconds * 1000
            )  # è½¬æ¢ä¸ºæ¯«ç§’

            # æ‰§è¡Œæ¸…ç†ä¸æ´»è·ƒownerè„šæœ¬
            result = await self._cleanup_inactive_owners_script(
                keys=[
                    self.owner_activate_time_zset_key,
                    self.queue_list_prefix,
                    self.queue_prefix,
                    self.counter_key,
                ],
                args=[
                    inactive_threshold,
                    current_time,
                    self.owner_expire_seconds,
                    self.FIXED_PARTITION_COUNT,
                ],
            )

            # è§£æè¿”å›ç»“æœ
            success, (cleaned_count, owner_count, assigned_partitions) = (
                self._parse_rebalance_result(result, 3)
            )
            if not success:
                return 0, 0, {}

            if cleaned_count > 0:
                logger.info(
                    "ğŸ§¹ RedisGroupQueueManager[%s] æ¸…ç†ä¸æ´»è·ƒownerå®Œæˆ: æ¸…ç†æ•°é‡=%d, å‰©ä½™owneræ•°é‡=%d, é‡æ–°åˆ†é…ç»“æœ=%s",
                    self.key_prefix,
                    cleaned_count,
                    owner_count,
                    assigned_partitions,
                )
            else:
                logger.debug(
                    "ğŸ§¹ RedisGroupQueueManager[%s] æ¸…ç†ä¸æ´»è·ƒownerå®Œæˆ: æ— éœ€æ¸…ç†",
                    self.key_prefix,
                )

            return cleaned_count, owner_count, assigned_partitions

        except (redis.RedisError, ValueError, TypeError) as e:
            logger.error(
                "âŒ RedisGroupQueueManager[%s] æ¸…ç†ä¸æ´»è·ƒownerå¤±è´¥: é”™è¯¯=%s",
                self.key_prefix,
                e,
            )
            return 0, 0, {}

    @rate_limit(max_rate=1, time_period=5, key_func=lambda: "force_cleanup_and_reset")
    async def force_cleanup_and_reset(self, purge_all: bool = False) -> int:
        """
        å¼ºåˆ¶æ¸…ç†å’Œé‡ç½®

        - purge_all=Falseï¼ˆé»˜è®¤ï¼‰ï¼šæ¸…ç† owner_activate_time_zset ä¸æ‰€æœ‰ owner çš„ queue_listï¼Œ
          ä¸åˆ é™¤å„åˆ†åŒºé˜Ÿåˆ—ï¼Œä»…é‡ç®—è®¡æ•°å™¨ã€‚
        - purge_all=Trueï¼šåœ¨ä¸Šè¿°åŸºç¡€ä¸Šé¢å¤–åˆ é™¤æ‰€æœ‰åˆ†åŒºé˜Ÿåˆ—ï¼Œå¹¶å°†è®¡æ•°å™¨ç½®0ï¼ˆå±é™©ï¼šå…¨é‡æ¸…åº“ï¼‰ã€‚

        Returns:
            int: å½“ purge_all=False æ—¶è¿”å›æ¸…ç†çš„ owner æ•°é‡ï¼›å½“ purge_all=True æ—¶è¿”å›åˆ é™¤çš„åˆ†åŒºæ•°é‡
        """
        try:
            await self._ensure_scripts_loaded()

            if purge_all:
                # å±é™©ï¼šæ¸…ç©ºæ‰€æœ‰åˆ†åŒºé˜Ÿåˆ— + owner + é‡ç½®è®¡æ•°å™¨ï¼ˆé€šè¿‡ç»Ÿä¸€è„šæœ¬ï¼Œpurge_all='1'ï¼‰
                purged_partitions = await self._force_cleanup_script(
                    keys=[
                        self.owner_activate_time_zset_key,
                        self.queue_list_prefix,
                        self.queue_prefix,
                        self.counter_key,
                    ],
                    args=[self.FIXED_PARTITION_COUNT, "1"],
                )

                # é‡ç½®æœ¬åœ°ç»Ÿè®¡
                async with self._stats_lock:
                    self._manager_stats.total_current_messages = 0
                    self._manager_stats.total_delivered_messages = 0
                    self._manager_stats.total_consumed_messages = 0
                    self._manager_stats.total_rejected_messages = 0

                logger.warning(
                    "ğŸ’¥ RedisGroupQueueManager[%s] å·²æ¸…ç©ºæ‰€æœ‰é˜Ÿåˆ—ä¸owner: åˆ†åŒºæ•°é‡=%d",
                    self.key_prefix,
                    purged_partitions,
                )
                return int(purged_partitions or 0)
            else:
                # ä»…é‡ç½®owneråŠé˜Ÿåˆ—åˆ†é…ï¼Œä¸åˆ é™¤åˆ†åŒºé˜Ÿåˆ—
                cleaned_count = await self._force_cleanup_script(
                    keys=[
                        self.owner_activate_time_zset_key,
                        self.queue_list_prefix,
                        self.queue_prefix,
                        self.counter_key,
                    ],
                    args=[self.FIXED_PARTITION_COUNT, "0"],
                )

                logger.warning(
                    "ğŸ’¥ RedisGroupQueueManager[%s] å¼ºåˆ¶æ¸…ç†å’Œé‡ç½®å®Œæˆ: æ¸…ç†owneræ•°é‡=%d",
                    self.key_prefix,
                    cleaned_count,
                )
                return cleaned_count

        except (redis.RedisError, ValueError, TypeError) as e:
            logger.error(
                "âŒ RedisGroupQueueManager[%s] å¼ºåˆ¶æ¸…ç†å’Œé‡ç½®å¤±è´¥: é”™è¯¯=%s",
                self.key_prefix,
                e,
            )
            return 0

    @rate_limit(max_rate=1, time_period=5, key_func=lambda: "get_stats")
    async def get_stats(
        self,
        group_key: Optional[str] = None,
        include_all_partitions: bool = False,
        include_partition_details: bool = False,
        include_consumer_info: bool = False,
    ) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆç»Ÿä¸€æ¥å£ï¼‰

        Args:
            group_key: åˆ†ç»„é”®ï¼Œå¦‚æœæä¾›åˆ™è·å–ç‰¹å®šé˜Ÿåˆ—ç»Ÿè®¡ï¼Œå¦åˆ™è·å–ç®¡ç†å™¨æ•´ä½“ç»Ÿè®¡
            include_all_partitions: æ˜¯å¦åŒ…å«æ‰€æœ‰åˆ†åŒºçš„ç»Ÿè®¡ä¿¡æ¯
            include_partition_details: æ˜¯å¦åŒ…å«åˆ†åŒºè¯¦ç»†ä¿¡æ¯
            include_consumer_info: æ˜¯å¦åŒ…å«æ¶ˆè´¹è€…ä¿¡æ¯

        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            await self._ensure_scripts_loaded()

            # å¦‚æœæŒ‡å®šäº†group_keyï¼Œè¿”å›ç‰¹å®šé˜Ÿåˆ—ç»Ÿè®¡
            if group_key is not None and not include_all_partitions:
                # è·å–å•ä¸ªåˆ†åŒºçš„ç»Ÿè®¡ä¿¡æ¯
                partition = self._hash_group_key_to_partition(group_key)
                queue_key = self._get_queue_key(partition)

                result = await self._get_stats_script(
                    keys=[queue_key, self.counter_key], args=[]
                )

                queue_size, _total_count, min_score, max_score = result

                return {
                    "type": "queue_stats",
                    "queue_name": f"{group_key}->partition={partition}",
                    "current_size": queue_size,
                    "last_activity_time": time.time(),
                    "min_score": min_score,
                    "max_score": max_score,
                    "partition": partition,
                }

            # è·å–æ‰€æœ‰åˆ†åŒºçš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç®¡ç†å™¨çº§åˆ«æˆ–å…¨åˆ†åŒºç»Ÿè®¡ï¼‰
            result = await self._get_all_partitions_stats_script(
                keys=[self.queue_prefix, self.counter_key],
                args=[str(self.FIXED_PARTITION_COUNT)],
            )

            (
                total_count,
                total_messages_in_queues,
                global_min_score,
                global_max_score,
                partition_stats_raw,
            ) = result

            # æ„å»ºåŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            async with self._stats_lock:
                # æ›´æ–°è¿è¡Œæ—¶é—´å’Œç»Ÿè®¡ä¿¡æ¯
                self._manager_stats.uptime_seconds = time.time() - self._start_time
                self._manager_stats.total_current_messages = total_messages_in_queues
                self._manager_stats.total_queues = self.FIXED_PARTITION_COUNT

                stats = self._manager_stats.to_dict()

            # æ·»åŠ å®æ—¶ç»Ÿè®¡ä¿¡æ¯
            stats.update(
                {
                    "type": (
                        "manager_stats" if group_key is None else "all_partitions_stats"
                    ),
                    "counter_total_count": total_count,
                    "actual_messages_in_queues": total_messages_in_queues,
                    "global_min_score": global_min_score,
                    "global_max_score": global_max_score,
                    "key_prefix": self.key_prefix,
                }
            )

            # å¦‚æœéœ€è¦æ¶ˆè´¹è€…ä¿¡æ¯
            if include_consumer_info:
                try:
                    active_owners_raw = await self.redis_client.zrange(
                        self.owner_activate_time_zset_key, 0, -1
                    )
                    # å®‰å…¨è§£ç owneråˆ—è¡¨
                    active_owners = [
                        self._safe_decode_redis_value(owner)
                        for owner in active_owners_raw
                    ]
                    stats["active_consumers_count"] = len(active_owners)
                    stats["active_consumers"] = active_owners

                    # è·å–åˆ†åŒºåˆ†é…æƒ…å†µ
                    partition_assignments = {}
                    for owner in active_owners:
                        queue_list_key = f"{self.queue_list_prefix}{owner}"
                        assigned_partitions_raw = await self.redis_client.lrange(
                            queue_list_key, 0, -1
                        )
                        # å®‰å…¨è§£ç åˆ†åŒºåˆ—è¡¨
                        assigned_partitions = [
                            self._safe_decode_redis_value(p)
                            for p in assigned_partitions_raw
                        ]
                        partition_assignments[owner] = assigned_partitions
                    stats["partition_assignments"] = partition_assignments

                except (redis.RedisError, ValueError, TypeError) as e:
                    logger.warning("è·å–æ¶ˆè´¹è€…ä¿¡æ¯å¤±è´¥: %s", e)
                    stats["active_consumers_count"] = 0
                    stats["active_consumers"] = []
                    stats["partition_assignments"] = {}

            # å¦‚æœéœ€è¦åˆ†åŒºè¯¦ç»†ä¿¡æ¯
            if include_partition_details:
                partitions = []
                non_empty_partitions = 0
                max_partition_size = 0
                min_partition_size = float('inf')

                for i in range(0, len(partition_stats_raw), 4):
                    if i + 3 < len(partition_stats_raw):
                        partition_size = partition_stats_raw[i + 1]
                        partitions.append(
                            {
                                "partition": self._safe_decode_redis_value(
                                    partition_stats_raw[i]
                                ),
                                "current_size": partition_size,
                                "min_score": partition_stats_raw[i + 2],
                                "max_score": partition_stats_raw[i + 3],
                            }
                        )

                        if partition_size > 0:
                            non_empty_partitions += 1
                            max_partition_size = max(max_partition_size, partition_size)
                            min_partition_size = min(min_partition_size, partition_size)

                stats["partitions"] = partitions
                stats["non_empty_partitions"] = non_empty_partitions
                stats["max_partition_size"] = (
                    max_partition_size if max_partition_size != 0 else 0
                )
                stats["min_partition_size"] = (
                    min_partition_size if min_partition_size != float('inf') else 0
                )

            return stats

        except (redis.RedisError, ValueError, TypeError) as e:
            logger.error("è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: group_key=%s, é”™è¯¯=%s", group_key, e)

            # é™çº§å¤„ç†ï¼šè¿”å›åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            try:
                current_count = await self.redis_client.get(self.counter_key)
                total_current_messages = int(current_count) if current_count else 0
            except (redis.RedisError, ValueError, TypeError):
                total_current_messages = 0

            return {
                "type": "error_fallback",
                "total_current_messages": total_current_messages,
                "total_queues": self.FIXED_PARTITION_COUNT,
                "error": str(e),
            }

    @rate_limit(
        max_rate=1,
        time_period=5,
        key_func=lambda group_key: f"get_queue_stats_{group_key}",
    )
    async def get_queue_stats(self, group_key: str) -> Optional[Dict[str, Any]]:
        """å…¼å®¹æ€§æ–¹æ³•ï¼šè·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯"""
        result = await self.get_stats(group_key=group_key)
        return result if result.get("type") != "error_fallback" else None

    @rate_limit(max_rate=1, time_period=5, key_func=lambda: "get_manager_stats")
    async def get_manager_stats(self) -> Dict[str, Any]:
        """å…¼å®¹æ€§æ–¹æ³•ï¼šè·å–ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return await self.get_stats()

    async def start(self):
        """
        å¯åŠ¨ç®¡ç†å™¨ï¼ˆå¼€å¯å‘¨æœŸä»»åŠ¡ï¼‰

        åªèƒ½å¯åŠ¨ä¸€æ¬¡ï¼Œshutdownåä¸èƒ½å†å¯åŠ¨

        Raises:
            RuntimeError: å¦‚æœç®¡ç†å™¨å·²ç»å¯åŠ¨æˆ–å·²å…³é—­
        """
        if self._state == ManagerState.STARTED:
            logger.warning(
                "âš ï¸ RedisGroupQueueManager[%s] å·²ç»å¯åŠ¨ï¼Œå¿½ç•¥é‡å¤å¯åŠ¨è¯·æ±‚",
                self.key_prefix,
            )
            return

        if self._state == ManagerState.SHUTDOWN:
            raise RuntimeError(
                f"RedisGroupQueueManager[{self.key_prefix}] å·²å…³é—­ï¼Œä¸èƒ½é‡æ–°å¯åŠ¨"
            )

        # çŠ¶æ€å¿…é¡»æ˜¯ CREATED
        if self._state != ManagerState.CREATED:
            raise RuntimeError(
                f"RedisGroupQueueManager[{self.key_prefix}] çŠ¶æ€å¼‚å¸¸: {self._state}"
            )

        logger.info("ğŸš€ RedisGroupQueueManager[%s] å¼€å§‹å¯åŠ¨...", self.key_prefix)

        await self.start_periodic_tasks()

        # æ›´æ–°çŠ¶æ€ä¸ºå·²å¯åŠ¨
        self._state = ManagerState.STARTED

        logger.info("âœ… RedisGroupQueueManager[%s] å¯åŠ¨å®Œæˆ", self.key_prefix)

    async def start_periodic_tasks(self):
        """å¯åŠ¨å®šæœŸä»»åŠ¡"""
        if self._running:
            return

        self._running = True

        # å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œä¸€æ¬¡æ¸…ç†
        try:
            await self.cleanup_inactive_owners()
            logger.info("ğŸ§¹ RedisGroupQueueManager[%s] å¯åŠ¨æ—¶æ¸…ç†å®Œæˆ", self.key_prefix)
        except (redis.RedisError, ValueError, TypeError) as e:
            logger.warning(
                "âš ï¸ RedisGroupQueueManager[%s] å¯åŠ¨æ—¶æ¸…ç†å¤±è´¥: %s", self.key_prefix, e
            )

        # å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œä¸€æ¬¡log
        try:
            await self._log_manager_details()
            logger.info(
                "ğŸ”¥ RedisGroupQueueManager[%s] å¯åŠ¨æ—¶æ—¥å¿—æ‰“å°å®Œæˆ", self.key_prefix
            )
        except (redis.RedisError, ValueError, TypeError) as e:
            logger.warning(
                "âš ï¸ RedisGroupQueueManager[%s] å¯åŠ¨æ—¶æ—¥å¿—æ‰“å°å¤±è´¥: %s",
                self.key_prefix,
                e,
            )

        # å¯åŠ¨å®šæœŸä»»åŠ¡
        self._log_task = asyncio.create_task(self._periodic_log_worker())
        self._cleanup_task = asyncio.create_task(self._periodic_cleanup_worker())

        logger.info("ğŸ“Š RedisGroupQueueManager[%s] å®šæœŸä»»åŠ¡å·²å¯åŠ¨", self.key_prefix)

    async def stop_periodic_tasks(self):
        """åœæ­¢å®šæœŸä»»åŠ¡"""
        if not self._running:
            return

        self._running = False

        # åœæ­¢æ—¥å¿—ä»»åŠ¡
        if self._log_task and not self._log_task.done():
            self._log_task.cancel()
            try:
                await self._log_task
            except asyncio.CancelledError:
                pass

        # åœæ­¢cleanupä»»åŠ¡
        if self._cleanup_task and not self._cleanup_task.done():
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass

        logger.info("ğŸ“Š RedisGroupQueueManager[%s] å®šæœŸä»»åŠ¡å·²åœæ­¢", self.key_prefix)

    async def _periodic_log_worker(self):
        """å®šæœŸæ—¥å¿—æ‰“å°å·¥ä½œåç¨‹"""
        try:
            while self._running:
                await asyncio.sleep(self.log_interval_seconds)
                if self._running:
                    await self._log_manager_details()
        except asyncio.CancelledError:
            logger.debug(
                "ğŸ“Š RedisGroupQueueManager[%s] å®šæœŸæ—¥å¿—ä»»åŠ¡è¢«å–æ¶ˆ", self.key_prefix
            )
        except (redis.RedisError, ValueError, TypeError) as e:
            logger.error(
                "ğŸ“Š RedisGroupQueueManager[%s] å®šæœŸæ—¥å¿—ä»»åŠ¡å¼‚å¸¸: %s", self.key_prefix, e
            )

    async def _periodic_cleanup_worker(self):
        """å®šæœŸæ¸…ç†å·¥ä½œåç¨‹"""
        try:
            while self._running:
                # æ·»åŠ ç›¸å¯¹æŠ–åŠ¨ï¼Œé¿å…æ‰€æœ‰å®ä¾‹åŒæ—¶æ¸…ç†ï¼Œå¹¶ç¡®ä¿éè´Ÿ
                jitter = self.cleanup_interval_seconds * 0.3
                delay = self.cleanup_interval_seconds + random.uniform(-jitter, jitter)
                await asyncio.sleep(max(1.0, delay))
                if self._running:
                    await self.cleanup_inactive_owners()
        except asyncio.CancelledError:
            logger.debug(
                "ğŸ§¹ RedisGroupQueueManager[%s] å®šæœŸæ¸…ç†ä»»åŠ¡è¢«å–æ¶ˆ", self.key_prefix
            )
        except (redis.RedisError, ValueError, TypeError) as e:
            logger.error(
                "ğŸ§¹ RedisGroupQueueManager[%s] å®šæœŸæ¸…ç†ä»»åŠ¡å¼‚å¸¸: %s", self.key_prefix, e
            )

    async def _log_manager_details(self):
        """æ‰“å°ç®¡ç†å™¨è¯¦ç»†ä¿¡æ¯"""
        try:
            manager_stats = await self.get_manager_stats()

            # æ‰“å°ç®¡ç†å™¨æ•´ä½“çŠ¶æ€
            logger.info(
                "ğŸ“Š RedisGroupQueueManager[%s] æ•´ä½“çŠ¶æ€: "
                "æ´»è·ƒé˜Ÿåˆ—=%d, æ€»æ¶ˆæ¯=%d, æ€»æŠ•é€’=%d, æ€»æ¶ˆè´¹=%d, æ€»æ‹’ç»=%d, è¿è¡Œæ—¶é—´=%.1fç§’",
                self.key_prefix,
                manager_stats["total_queues"],
                manager_stats["total_current_messages"],
                manager_stats["total_delivered_messages"],
                manager_stats["total_consumed_messages"],
                manager_stats["total_rejected_messages"],
                manager_stats["uptime_seconds"],
            )

            # ç»Ÿä¸€ä¸€æ¬¡æ€§æ‰“å°æ‰€æœ‰åˆ†åŒºçš„è¯¦ç»†ä¿¡æ¯
            partitions = self.partition_names
            details_lines = []
            for partition in partitions:
                try:
                    queue_key = self._get_queue_key(partition)
                    queue_size = await self.redis_client.zcard(queue_key)
                    if queue_size > 0:
                        # è·å–æœ€å°å’Œæœ€å¤§åˆ†æ•°
                        min_result = await self.redis_client.zrange(
                            queue_key, 0, 0, withscores=True
                        )
                        max_result = await self.redis_client.zrange(
                            queue_key, -1, -1, withscores=True
                        )
                        min_score = min_result[0][1] if min_result else 0
                        max_score = max_result[0][1] if max_result else 0
                        details_lines.append(
                            f"   åˆ†åŒº[{partition}]: å¤§å°={queue_size}, åˆ†æ•°èŒƒå›´=[{min_score:.3f}, {max_score:.3f}]"
                        )
                    else:
                        details_lines.append(f"   åˆ†åŒº[{partition}]: å¤§å°=0")
                except (redis.RedisError, ValueError, TypeError) as e:
                    details_lines.append(f"   åˆ†åŒº[{partition}]: è·å–çŠ¶æ€å¤±è´¥: {e}")

            if details_lines:
                logger.info(
                    "ğŸ”¥ åˆ†åŒºçŠ¶æ€æ±‡æ€»: å…±%dä¸ªåˆ†åŒº\n%s",
                    len(partitions),
                    "\n".join(details_lines),
                )

        except (redis.RedisError, ValueError, TypeError) as e:
            logger.error(
                "ğŸ“Š RedisGroupQueueManager[%s] æ‰“å°è¯¦æƒ…å¤±è´¥: %s", self.key_prefix, e
            )

    async def shutdown(self, mode: ShutdownMode = ShutdownMode.HARD) -> bool:
        """
        å…³é—­ç®¡ç†å™¨

        Args:
            mode: å…³é—­æ¨¡å¼

        Returns:
            bool: æ˜¯å¦æˆåŠŸå…³é—­
        """
        if self._state == ManagerState.SHUTDOWN:
            logger.warning(
                "âš ï¸ RedisGroupQueueManager[%s] å·²ç»å…³é—­ï¼Œå¿½ç•¥é‡å¤å…³é—­è¯·æ±‚",
                self.key_prefix,
            )
            return True

        if self._state == ManagerState.CREATED:
            logger.info(
                "â„¹ï¸ RedisGroupQueueManager[%s] æœªå¯åŠ¨çŠ¶æ€ä¸‹å…³é—­", self.key_prefix
            )
            self._state = ManagerState.SHUTDOWN
            return True

        # çŠ¶æ€å¿…é¡»æ˜¯ STARTED
        if self._state != ManagerState.STARTED:
            logger.warning(
                "âš ï¸ RedisGroupQueueManager[%s] çŠ¶æ€å¼‚å¸¸ï¼Œå¼ºåˆ¶å…³é—­: %s",
                self.key_prefix,
                self._state,
            )

        logger.info("ğŸ”Œ RedisGroupQueueManager[%s] å¼€å§‹å…³é—­...", self.key_prefix)

        # åœæ­¢å®šæœŸä»»åŠ¡
        await self.stop_periodic_tasks()

        if mode == ShutdownMode.SOFT:
            # è½¯æ€§å…³é—­ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ¶ˆæ¯
            stats = await self.get_manager_stats()
            remaining_messages = stats.get("total_current_messages", 0)
            if remaining_messages > 0:
                logger.warning(
                    "âš ï¸ RedisGroupQueueManager[%s] è½¯æ€§å…³é—­æ£€æµ‹åˆ°å‰©ä½™æ¶ˆæ¯: %dæ¡",
                    self.key_prefix,
                    remaining_messages,
                )
                # è½¯æ€§å…³é—­å¤±è´¥ï¼Œä½†ä¸æ”¹å˜çŠ¶æ€ï¼Œå…è®¸é‡è¯•
                return False

        # å…³é—­å‰æœ€åä¸€æ¬¡æ‰“å°è¯¦ç»†ä¿¡æ¯
        try:
            await self._log_manager_details()
            logger.info(
                "ğŸ”¥ RedisGroupQueueManager[%s] å…³é—­å‰æœ€ç»ˆçŠ¶æ€æ—¥å¿—å®Œæˆ", self.key_prefix
            )
        except (redis.RedisError, ValueError, TypeError) as e:
            logger.warning(
                "âš ï¸ RedisGroupQueueManager[%s] å…³é—­å‰æ—¥å¿—æ‰“å°å¤±è´¥: %s",
                self.key_prefix,
                e,
            )

        # æ›´æ–°çŠ¶æ€ä¸ºå·²å…³é—­
        self._state = ManagerState.SHUTDOWN

        logger.info("ğŸ”Œ RedisGroupQueueManager[%s] å·²å…³é—­", self.key_prefix)
        return True

    def get_state(self) -> ManagerState:
        """
        è·å–ç®¡ç†å™¨å½“å‰çŠ¶æ€

        Returns:
            ManagerState: å½“å‰çŠ¶æ€
        """
        return self._state

    def __repr__(self) -> str:
        return (
            f"RedisGroupQueueManager(key_prefix={self.key_prefix}, "
            f"max_total_messages={self.max_total_messages})"
        )
