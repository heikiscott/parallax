# Token Statistics åŠŸèƒ½æ–‡æ¡£

## æ¦‚è¿°

Token Statistics åŠŸèƒ½ç”¨äºç»Ÿè®¡å’Œè·Ÿè¸ªè¯„æµ‹è¿‡ç¨‹ä¸­å„é˜¶æ®µçš„ LLM token ä½¿ç”¨æƒ…å†µï¼Œå¸®åŠ©åˆ†ææˆæœ¬å’Œæ€§èƒ½ã€‚

## æ ¸å¿ƒç‰¹æ€§

### âœ… è‡ªåŠ¨æ”¶é›†
- åœ¨ `LLMProvider.generate()` å±‚é¢è‡ªåŠ¨æ”¶é›†ï¼Œæ— éœ€æ‰‹åŠ¨æ·»åŠ ç»Ÿè®¡ä»£ç 
- ä½¿ç”¨å›è°ƒæœºåˆ¶ï¼Œä½è€¦åˆè®¾è®¡
- æ”¯æŒæ‰€æœ‰ä½¿ç”¨ `LLMProvider` çš„ä»£ç è·¯å¾„

### âœ… é˜¶æ®µè¿½è¸ª
- ä½¿ç”¨ Python `contextvars` è‡ªåŠ¨æ¨æ–­å½“å‰é˜¶æ®µ
- æ”¯æŒçš„é˜¶æ®µï¼š`add`, `cluster`, `search`, `answer`
- å¯æ‰©å±•åˆ°è‡ªå®šä¹‰é˜¶æ®µ

### âœ… é²æ£’æ€§
- æ— è®º LangGraph workflow å¦‚ä½•å˜åŒ–ï¼Œéƒ½èƒ½æ­£ç¡®ç»Ÿè®¡
- å‘åå…¼å®¹ï¼Œä¸å½±å“ä¸ä½¿ç”¨ç»Ÿè®¡çš„åœºæ™¯
- æ•è·æ‰€æœ‰ LLM è°ƒç”¨ï¼ˆåŒ…æ‹¬ EventLogExtractorã€é—®é¢˜åˆ†ç±»ã€æŸ¥è¯¢æ”¹å†™ç­‰ï¼‰

## æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Workflow Node                         â”‚
â”‚  (è®¾ç½®é˜¶æ®µ: TokenStatsCollector.set_current_stage())   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LLMProvider                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ async def generate():                            â”‚  â”‚
â”‚  â”‚   result = await self.provider.generate(...)    â”‚  â”‚
â”‚  â”‚   if self.stats_callback:                       â”‚  â”‚
â”‚  â”‚       stats = get_current_call_stats()          â”‚  â”‚
â”‚  â”‚       self.stats_callback(stats)  # è‡ªåŠ¨å›è°ƒ   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             TokenStatsCollector                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ def record(stats):                               â”‚  â”‚
â”‚  â”‚   stage = _current_stage.get()  # ä»ä¸Šä¸‹æ–‡è·å– â”‚  â”‚
â”‚  â”‚   self.stage_stats[stage].append(stats)         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬ä½¿ç”¨ï¼ˆå·²è‡ªåŠ¨å¯ç”¨ï¼‰

è¿è¡Œè¯„æµ‹æ—¶ï¼Œtoken ç»Ÿè®¡å·²é»˜è®¤å¯ç”¨ï¼š

```bash
python eval/cli.py --dataset locomo-mini --system parallax
```

è¯„æµ‹å®Œæˆåä¼šè¾“å‡ºï¼š

```
======================================================================
ğŸ“Š Token Usage Statistics by Stage
======================================================================

ğŸ”¹ Add (MemUnit Extraction)
   Total LLM Calls:      150
   Total Tokens:         1,234,567
     - Prompt Tokens:    987,654
     - Completion Tokens: 246,913
   Avg Tokens per Call:  8,230.4
     - Avg Prompt:       6,584.4
     - Avg Completion:   1,646.1

ğŸ”¹ Search (Query Classification/Rewrite)
   Total LLM Calls:      45
   Total Tokens:         123,456
     - Prompt Tokens:    98,765
     - Completion Tokens: 24,691
   Avg Tokens per Call:  2,745.7
     - Avg Prompt:       2,195.0
     - Avg Completion:   548.7

ğŸ”¹ Answer (Response Generation)
   Total LLM Calls:      30
   Total Tokens:         456,789
     - Prompt Tokens:    345,678
     - Completion Tokens: 111,111
   Avg Tokens per Call:  15,226.3
     - Avg Prompt:       11,522.6
     - Avg Completion:   3,703.7

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ˆ Overall Summary
   Total LLM Calls:      225
   Total Tokens:         1,814,812
   Avg Tokens per Call:  8,066.3

======================================================================
```

ç»Ÿè®¡ç»“æœä¹Ÿä¼šä¿å­˜åˆ° `eval/results/{dataset}-{system}/token_stats.json`ã€‚

### 2. åœ¨è‡ªå®šä¹‰ Workflow Node ä¸­ä½¿ç”¨

å¦‚æœä½ åˆ›å»ºäº†è‡ªå®šä¹‰çš„ LangGraph workflow nodeï¼Œåªéœ€è®¾ç½®å½“å‰é˜¶æ®µï¼š

```python
from eval.utils.token_stats import TokenStatsCollector
from src.orchestration.nodes import register_node

@register_node("my_custom_stage")
async def my_custom_stage_node(state, context):
    """è‡ªå®šä¹‰é˜¶æ®µ"""
    # è®¾ç½®å½“å‰é˜¶æ®µï¼ˆç”¨äº token ç»Ÿè®¡ï¼‰
    TokenStatsCollector.set_current_stage("my_custom_stage")

    try:
        # è°ƒç”¨ä»»ä½•ä½¿ç”¨ llm_provider çš„ä»£ç 
        # æ‰€æœ‰ LLM è°ƒç”¨éƒ½ä¼šè‡ªåŠ¨è¢«å½’ç±»åˆ° "my_custom_stage"
        result = await some_function(context.llm_provider)

        return {
            "result": result,
            "completed_stages": ["my_custom_stage"]
        }
    finally:
        # æ¸…ç†é˜¶æ®µæ ‡è®°
        TokenStatsCollector.set_current_stage(None)
```

### 3. æ‰‹åŠ¨æŒ‡å®šé˜¶æ®µï¼ˆä¸æ¨èï¼‰

å¦‚æœç¡®å®éœ€è¦æ‰‹åŠ¨æŒ‡å®šé˜¶æ®µï¼ˆç»•è¿‡ context variableï¼‰ï¼Œå¯ä»¥ï¼š

```python
# åœ¨ adapter æˆ–å…¶ä»–ä»£ç ä¸­
if self.token_stats_collector:
    stats = self.llm_provider.get_current_call_stats()
    self.token_stats_collector.record(stage="custom_stage", stats=stats)
```

**æ³¨æ„ï¼š** è¿™ç§æ–¹å¼éœ€è¦æ‰‹åŠ¨åœ¨æ¯ä¸ª LLM è°ƒç”¨åæ·»åŠ ä»£ç ï¼Œä¸æ¨èä½¿ç”¨ã€‚ä¼˜å…ˆä½¿ç”¨æ–¹æ³• 2ï¼ˆè®¾ç½® context variableï¼‰ã€‚

## æ•°æ®æ ¼å¼

### JSON è¾“å‡ºæ ¼å¼

`token_stats.json` æ–‡ä»¶æ ¼å¼ï¼š

```json
{
  "summaries": {
    "add": {
      "total_calls": 150,
      "total_prompt_tokens": 987654,
      "total_completion_tokens": 246913,
      "total_tokens": 1234567,
      "avg_prompt_tokens": 6584.4,
      "avg_completion_tokens": 1646.1,
      "avg_total_tokens": 8230.4
    },
    "search": {
      "total_calls": 45,
      "total_prompt_tokens": 98765,
      "total_completion_tokens": 24691,
      "total_tokens": 123456,
      "avg_prompt_tokens": 2195.0,
      "avg_completion_tokens": 548.7,
      "avg_total_tokens": 2745.7
    },
    "answer": {
      "total_calls": 30,
      "total_prompt_tokens": 345678,
      "total_completion_tokens": 111111,
      "total_tokens": 456789,
      "avg_prompt_tokens": 11522.6,
      "avg_completion_tokens": 3703.7,
      "avg_total_tokens": 15226.3
    }
  },
  "raw_data": {
    "add": [
      {"prompt_tokens": 5000, "completion_tokens": 1500, "total_tokens": 6500},
      {"prompt_tokens": 5200, "completion_tokens": 1600, "total_tokens": 6800},
      ...
    ],
    "search": [...],
    "answer": [...]
  }
}
```

## å®ç°ç»†èŠ‚

### æ ¸å¿ƒç±»

#### 1. `TokenStatsCollector` (eval/utils/token_stats.py)

```python
class TokenStatsCollector:
    def record(self, stage: Optional[str] = None, stats: Optional[Dict] = None):
        """è®°å½• token ä½¿ç”¨æƒ…å†µ"""

    def get_stage_summary(self, stage: str) -> Dict:
        """è·å–æŒ‡å®šé˜¶æ®µçš„ç»Ÿè®¡æ‘˜è¦"""

    def generate_report(self) -> str:
        """ç”Ÿæˆå¯è¯»çš„ç»Ÿè®¡æŠ¥å‘Š"""

    def save_to_json(self, filepath: str):
        """ä¿å­˜ç»Ÿè®¡æ•°æ®åˆ° JSON æ–‡ä»¶"""

    @staticmethod
    def set_current_stage(stage: Optional[str]):
        """è®¾ç½®å½“å‰é˜¶æ®µï¼ˆä½¿ç”¨ contextvarsï¼‰"""
```

#### 2. `LLMProvider` (src/providers/llm/llm_provider.py)

```python
class LLMProvider:
    def __init__(
        self,
        provider_type: str,
        enable_stats: bool = False,
        stats_callback: Optional[Callable[[dict], None]] = None,
        **kwargs
    ):
        """åˆå§‹åŒ– LLM Providerï¼Œæ”¯æŒç»Ÿè®¡å›è°ƒ"""

    async def generate(self, prompt, ...):
        """ç”Ÿæˆæ–‡æœ¬ï¼Œè‡ªåŠ¨è°ƒç”¨ç»Ÿè®¡å›è°ƒ"""
        result = await self.provider.generate(...)

        # è‡ªåŠ¨æ”¶é›†ç»Ÿè®¡
        if self.enable_stats and self.stats_callback:
            stats = self.provider.get_current_call_stats()
            if stats:
                self.stats_callback(stats)

        return result
```

### Context Variable æœºåˆ¶

ä½¿ç”¨ Python çš„ `contextvars` æ¨¡å—æ¥è·Ÿè¸ªå½“å‰é˜¶æ®µï¼Œè¿™æ˜¯çº¿ç¨‹å®‰å…¨å’Œå¼‚æ­¥å®‰å…¨çš„ï¼š

```python
import contextvars

_current_stage = contextvars.ContextVar('current_stage', default=None)

# åœ¨ workflow node ä¸­è®¾ç½®
TokenStatsCollector.set_current_stage("answer")

# åœ¨ callback ä¸­è‡ªåŠ¨è·å–
stage = _current_stage.get()  # è¿”å› "answer"
```

**ä¼˜åŠ¿ï¼š**
- å¼‚æ­¥å®‰å…¨ï¼šæ¯ä¸ªå¼‚æ­¥ä»»åŠ¡æœ‰ç‹¬ç«‹çš„ä¸Šä¸‹æ–‡
- æ— éœ€æ‰‹åŠ¨ä¼ é€’ï¼šè‡ªåŠ¨ç»§æ‰¿åˆ°å­è°ƒç”¨
- è‡ªåŠ¨æ¸…ç†ï¼šä½¿ç”¨ try-finally ç¡®ä¿æ¸…ç†

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆæœ‰äº› LLM è°ƒç”¨è¢«å½’ç±»åˆ° "unknown"ï¼Ÿ

**A:** è¿™è¯´æ˜è°ƒç”¨å‘ç”Ÿæ—¶æ²¡æœ‰è®¾ç½® stageã€‚æ£€æŸ¥ï¼š
1. æ˜¯å¦åœ¨ workflow node ä¸­è°ƒç”¨äº† `TokenStatsCollector.set_current_stage()`
2. æ˜¯å¦åœ¨ finally å—ä¸­æ¸…ç†äº† stage
3. æ˜¯å¦åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­æ­£ç¡®ä¼ é€’äº† context variable

### Q2: å¦‚ä½•ç¦ç”¨ token ç»Ÿè®¡ï¼Ÿ

**A:** åœ¨ `cli.py` ä¸­ä¿®æ”¹ï¼š

```python
# å°† enable_token_stats=True æ”¹ä¸º False
adapter = create_adapter(
    system_config["adapter"],
    system_config,
    output_dir=output_dir,
    enable_token_stats=False  # ç¦ç”¨ç»Ÿè®¡
)
```

### Q3: ç»Ÿè®¡ä¼šå½±å“æ€§èƒ½å—ï¼Ÿ

**A:** å‡ ä¹æ²¡æœ‰å½±å“ï¼š
- ç»Ÿè®¡æ”¶é›†æ˜¯çº¯å†…å­˜æ“ä½œï¼ˆå­—å…¸è¿½åŠ ï¼‰
- ä»…åœ¨ LLM è°ƒç”¨å®Œæˆåæ‰§è¡Œï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
- Overhead < 1ms per call

### Q4: å¦‚ä½•ä¸ºæ–°çš„è‡ªå®šä¹‰ workflow æ·»åŠ ç»Ÿè®¡ï¼Ÿ

**A:** éµå¾ªä»¥ä¸‹æ¨¡æ¿ï¼š

```python
@register_node("my_stage")
async def my_stage_node(state, context):
    TokenStatsCollector.set_current_stage("my_stage")
    try:
        # ä½ çš„ä»£ç 
        result = await my_function(context.llm_provider)
        return {"result": result}
    finally:
        TokenStatsCollector.set_current_stage(None)
```

## æ‰©å±•å’Œå®šåˆ¶

### æ·»åŠ æ–°çš„ç»Ÿè®¡ç»´åº¦

ä¿®æ”¹ `TokenStatsCollector.record()` æ¥æ”¶é›†é¢å¤–ä¿¡æ¯ï¼š

```python
def record(self, stage: Optional[str] = None, stats: Optional[Dict] = None) -> None:
    # ... ç°æœ‰ä»£ç  ...

    self.stage_stats[stage].append({
        "prompt_tokens": stats.get("prompt_tokens", 0),
        "completion_tokens": stats.get("completion_tokens", 0),
        "total_tokens": stats.get("total_tokens", 0),
        # æ–°å¢ï¼šè®°å½•æ—¶é—´æˆ³
        "timestamp": time.time(),
        # æ–°å¢ï¼šè®°å½•æ¨¡å‹
        "model": stats.get("model", "unknown"),
    })
```

### è‡ªå®šä¹‰æŠ¥å‘Šæ ¼å¼

ä¿®æ”¹ `TokenStatsCollector.generate_report()` æ¥å®šåˆ¶è¾“å‡ºæ ¼å¼ã€‚

## æ€»ç»“

Token Statistics åŠŸèƒ½æä¾›äº†ä¸€ä¸ª**é²æ£’ã€ä½è€¦åˆã€æ˜“æ‰©å±•**çš„æ–¹æ¡ˆæ¥è¿½è¸ª LLM token ä½¿ç”¨æƒ…å†µï¼š

âœ… **é›¶ä¾µå…¥**ï¼šæ— éœ€åœ¨ä¸šåŠ¡ä»£ç ä¸­æ‰‹åŠ¨æ·»åŠ ç»Ÿè®¡
âœ… **è‡ªåŠ¨åŒ–**ï¼šé€šè¿‡å›è°ƒå’Œ context variable è‡ªåŠ¨æ”¶é›†
âœ… **å¯æ‰©å±•**ï¼šæ”¯æŒä»»æ„è‡ªå®šä¹‰ workflow å’Œé˜¶æ®µ
âœ… **é²æ£’æ€§**ï¼šæ— è®ºæµç¨‹å¦‚ä½•å˜åŒ–ï¼Œéƒ½èƒ½æ­£ç¡®ç»Ÿè®¡

---

**æœ€åæ›´æ–°ï¼š** 2025-12-10
**ä½œè€…ï¼š** Claude Code
