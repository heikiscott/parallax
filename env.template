# =====================================================
# Memory System Configuration Template
# 记忆系统配置模板
# =====================================================
#
# SECURITY NOTICE / 安全提示:
# - Copy this file to .env and fill in your actual API keys
# - Never commit .env file to version control
# - Keep your API keys secure and private
# - 将此文件复制为.env并填入您的实际API密钥
# - 永远不要将.env文件提交到版本控制
# - 保护好您的API密钥安全
#
# SETUP INSTRUCTIONS / 设置说明:
# 1. cp env.template .env
# 2. Edit .env with your actual values
# 3. The system will automatically load these values
# =====================================================


# ===================
# LLM Configuration / LLM配置
# ===================

LLM_PROVIDER=openai
# evaluation 请用 openai/gpt-4.1-mini 以保证结果可复现性，demo 可使用 x-ai/grok-4-fast 平衡速度/成本/效果
LLM_MODEL=x-ai/grok-4-fast
LLM_BASE_URL=https://openrouter.ai/api/v1
LLM_API_KEY=sk-or-v1-xxxx
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=32768
# openrouter/其他的供应商设置，默认为 default，用 openrouter 的 qwen3 时建议设置为 cerebras
# LLM_OPENROUTER_PROVIDER=cerebras

# OpenAI Provider Configuration / OpenAI Provider配置
OPENAI_TIMEOUT=120                     # API超时(秒)
OPENAI_MAX_RETRIES=0                   # SDK层重试次数（让SDK处理临时错误）

# Tenacity retry configuration / Tenacity重试配置
# 使用指数退避策略自动处理速率限制和临时错误
OPENAI_RETRY_MIN_WAIT=5                 # 重试最小等待时间(秒)
OPENAI_RETRY_MAX_WAIT=300               # 重试最大等待时间(秒)
OPENAI_RETRY_ATTEMPTS=20                # 最大重试次数

# Multi-layer rate limiting / 多层限流控制
# Layer 0: HTTPX连接池 (防止底层TCP连接耗尽，自动配置为 max_concurrent * 4)
# Layer 1: 物理并发控制 (防止请求洪峰)
OPENAI_MAX_CONCURRENT=20               # Provider最大并发数

# Layer 2: 主动速率限制 (防止 429 错误)
# ⚠️  重要：设置为你的 OpenAI tier 限制的 80% 以留有余地
# Free tier: ~60 RPM  -> 建议设置 48
# Tier 1: ~500 RPM    -> 建议设置 400
# Tier 2: ~5000 RPM   -> 建议设置 4000
OPENAI_RPM_LIMIT=200                   # 每分钟请求数限制 (proactive throttling)


# ===================
# DeepInfra Integration / DeepInfra集成
# ===================

# DeepInfra Embedding Service / DeepInfra嵌入服务配置
# 支持使用本地部署的模型或DeepInfra API
DEEPINFRA_API_KEY=xxxxx
DEEPINFRA_BASE_URL=https://api.deepinfra.com/v1/openai
DEEPINFRA_EMBEDDING_MODEL=Qwen/Qwen3-Embedding-4B
DEEPINFRA_TIMEOUT=30
DEEPINFRA_MAX_RETRIES=3
DEEPINFRA_BATCH_SIZE=10
DEEPINFRA_MAX_CONCURRENT=5
DEEPINFRA_ENCODING_FORMAT=float
DEEPINFRA_DIMENSIONS=1024

# DeepInfra Rerank Service / DeepInfra重排序服务配置
DEEPINFRA_RERANK_BASE_URL=https://api.deepinfra.com/v1/inference
DEEPINFRA_RERANK_MODEL=Qwen/Qwen3-Reranker-4B
DEEPINFRA_RERANK_TIMEOUT=30
DEEPINFRA_RERANK_MAX_RETRIES=3
DEEPINFRA_RERANK_BATCH_SIZE=10
DEEPINFRA_RERANK_MAX_CONCURRENT=5


# ===================
# Redis Configuration / Redis配置
# ===================

REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=8
REDIS_SSL=false

# ===================
# MongoDB Configuration / MongoDB配置
# ===================

MONGODB_HOST=localhost
MONGODB_PORT=27017
MONGODB_USERNAME=admin
MONGODB_PASSWORD=memsys123
MONGODB_DATABASE=memsys
MONGODB_URI_PARAMS=socketTimeoutMS=15000&authSource=admin

# ===================
# Elasticsearch Configuration / Elasticsearch配置
# ===================

ES_HOSTS=http://localhost:19200
ES_USERNAME=
ES_PASSWORD=
ES_VERIFY_CERTS=false
SELF_ES_INDEX_NS=memsys

# ===================
# Milvus Configuration / Milvus向量数据库配置
# ===================

MILVUS_HOST=localhost
MILVUS_PORT=19530
SELF_MILVUS_COLLECTION_NS=memsys

# ===================
# API Server Configuration / API服务器配置
# ===================

# V3 API Base URL (用于 chat_with_memory.py 等客户端)
API_BASE_URL=http://localhost:8001

# ===================
# Evaluation Configuration / 评估配置
# ===================

# Answer stage batch processing / Answer 阶段批处理配置
# Batch 只负责批次管理，超时和并发由 Provider 层控制
EVAL_ANSWER_BATCH_SIZE=20              # 每批次处理的请求数（batch间串行，batch内并发）
EVAL_ANSWER_BATCH_DELAY=3              # batch间延迟（秒），用于主动降低请求频率
EVAL_ANSWER_REQUEST_INTERVAL=0.5       # batch内请求间隔（秒），控制请求发送速率

# Concurrency settings for each evaluation stage (避免API限流)
# Recommended for OpenAI free tier: 3-5, paid tier: 10-20

# MemUnit extraction concurrency (LLM-intensive, most strict)
EVAL_EXTRACTION_MAX_CONCURRENT=5

# Index building concurrency (Embedding API, batch processing)
EVAL_INDEXING_MAX_CONCURRENT=5

# Memory retrieval concurrency (includes Agentic retrieval with LLM)
EVAL_RETRIEVAL_MAX_CONCURRENT=5

# Response generation concurrency (LLM answer generation)
EVAL_RESPONSE_MAX_CONCURRENT=5

# LLM-as-a-Judge evaluation concurrency
EVAL_JUDGMENT_MAX_CONCURRENT=5

# ===================
# Environment & Logging / 环境与日志配置
# ===================

LOG_LEVEL=INFO
ENV=dev
PYTHONASYNCIODEBUG=1
MEMORY_LANGUAGE=zh
